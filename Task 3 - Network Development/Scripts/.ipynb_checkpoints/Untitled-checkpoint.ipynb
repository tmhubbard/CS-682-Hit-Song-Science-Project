{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This neural network was written by Trevor Hubbard and Phil Sifferlin. It performs Hit Song Prediction using vector embeddings of a music-industry co-collaboration network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some import statements\n",
    "import torch, json, random, time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Setting up Pytorch's use of CUDA \n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASSES**\n",
    "\n",
    "*The next couple cells define the classes we'll use throughout this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class will be used to help in the song data! \n",
    "class SongDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # The init method defines how we'll input the data; \n",
    "    def __init__(self, embeddingTsvPath, audioJsonPath):\n",
    "\n",
    "        # This dictionary will hold all of the data\n",
    "        self.songDict = {}\n",
    "\n",
    "        curTime = time.time()\n",
    "        print(\"\\nReading the network embeddings from the .tsv...\")\n",
    "\n",
    "        # Open the tsv and process the data in it\n",
    "        with open(embeddingTsvPath, \"r\", encoding=\"utf-8\") as tsvFile:\n",
    "\n",
    "            # Iterate through each line of the .tsv and store the info\n",
    "            hitCol = 0\n",
    "            idCol = 0\n",
    "            for lineNum, line in enumerate(tsvFile):\n",
    "\n",
    "                line = line.strip()\n",
    "                splitLine = line.split(\"\\t\")\n",
    "\n",
    "                # If we're on the first line, figure out where the \"hit\" column is\n",
    "                if (lineNum == 0): \n",
    "                    for idx, header in enumerate(splitLine):\n",
    "                        if (header == \"songID\"):\n",
    "                            idCol = idx\n",
    "                        if (header == \"hit\"):\n",
    "                            hitCol = idx\n",
    "                    continue\n",
    "\n",
    "                # Add the song to the songDict\n",
    "                songID = int(splitLine[idCol])\n",
    "                curHit = int(splitLine[hitCol])\n",
    "                if (not songID in self.songDict):\n",
    "                    self.songDict[songID] = {\"hit\": curHit, \"embedding\": None, \"audio features\": None}\n",
    "\n",
    "                # Update the song's embedding \n",
    "                self.songDict[songID][\"embedding\"] = torch.tensor([float(x) for x in splitLine[hitCol+1:]], dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "        print(\"Finished reading in the embeddings! It took %.3f seconds\" % (time.time()-curTime))\n",
    "        curTime = time.time()\n",
    "        print(\"\\nReading in the Spotify data from the .json...\")\n",
    "\n",
    "        # Open the audio features JSON and process the data in it\n",
    "        with open(audioJsonPath, \"r\", encoding=\"utf-8\") as jsonFile:\n",
    "            songData = json.load(jsonFile)[\"songs\"]\n",
    "            features = ['duration_ms', 'key', 'mode', 'time_signature', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'valence', 'tempo']\n",
    "            for song in songData:\n",
    "                songID = int(song[\"title\"][1])\n",
    "                curHit = int(song[\"hit\"])\n",
    "                if (song[\"audio_features\"] not in [{}, None]):\n",
    "                    if (songID not in self.songDict):\n",
    "                        self.songDict[songID] = {\"hit\": curHit, \"embedding\": None, \"audio features\": None}\n",
    "                    self.songDict[songID][\"audio features\"] = torch.tensor([song['audio_features'][feature] for feature in features], dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "        print(\"Finished reading in the .json! It took %.3f seconds\" % (time.time()-curTime))\n",
    "\n",
    "        # Remove any songs that don't have both an embedding and audio features\n",
    "        curTime = time.time()\n",
    "        print(\"\\nRemoving songs without both an embedding and audio features...\")\n",
    "        hitCount = 0\n",
    "        idsToRemove = []\n",
    "        for songNum, songID in enumerate(self.songDict.keys()):\n",
    "            song = self.songDict[songID]\n",
    "            if ((song[\"embedding\"] is None) or (song[\"audio features\"] is None)):\n",
    "                idsToRemove.append(songID)\n",
    "                continue\n",
    "            else:\n",
    "                if (song[\"hit\"] == 1): hitCount += 1\n",
    "        for songID in idsToRemove:\n",
    "            del self.songDict[songID]\n",
    "        print(\"Finished removing the songs! It took %.3f seconds.\" % (time.time()-curTime))\n",
    "\n",
    "        # Creating the songList (a list version of the songDict)\n",
    "        shuffledSongDict = list(self.songDict.keys())\n",
    "        np.random.shuffle(shuffledSongDict)\n",
    "        self.songList = []\n",
    "        nonHitCount = 0\n",
    "        for songID in shuffledSongDict:\n",
    "            song = self.songDict[songID]\n",
    "\n",
    "            # Skip if this is a nonHit and we've already added all of those\n",
    "            if (nonHitCount == hitCount and song[\"hit\"] == 0):\n",
    "                continue\n",
    "\n",
    "            self.songList.append(song)\n",
    "            self.songList[-1][\"id\"] = songID\n",
    "\n",
    "            if (song[\"hit\"] == 0):\n",
    "                nonHitCount += 1\n",
    "\n",
    "    # The len method returns the length of x_data\n",
    "    def __len__(self):\n",
    "        return len(self.songList)\n",
    "\n",
    "    # The getitem method will specify how to return a particular index\n",
    "    def __getitem__(self, idx):\n",
    "        if (torch.is_tensor(idx)):\n",
    "            idx = idx.tolist()\n",
    "        song = self.songList[idx]\n",
    "        songID = song[\"id\"]\n",
    "        emb = song[\"embedding\"]\n",
    "        audio_features = song[\"audio features\"]\n",
    "        hit = torch.tensor(song[\"hit\"], dtype=torch.float32, device=\"cuda\")\n",
    "        return (songID, emb, audio_features, hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is the Two Layer Net that we use for the audio features model**CLASSES**\n",
    "class TwoLayerFC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        scores = self.fc2(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHODS**\n",
    "\n",
    "*The next couple cells define the methods we'll use throughout this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will check the accuracy of the model using data from the loader\n",
    "def checkAccuracy(loader, model, modelType):\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (songID, emb, audio_features, hit) in loader:\n",
    "            binaryScores = []\n",
    "            if (modelType == \"emb\"):\n",
    "                binaryScores = torch.round(torch.sigmoid(model(emb))).reshape(hit.shape)\n",
    "            elif (modelType == \"audio\"):\n",
    "                binaryScores = torch.round(torch.sigmoid(model(audio_features))).reshape(hit.shape)\n",
    "            num_correct += (hit == binaryScores).sum().float()\n",
    "            num_samples += len(hit)\n",
    "        return (num_correct/num_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Dataset from the song embedding .tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading the network embeddings from the .tsv...\n",
      "Finished reading in the embeddings! It took 21.680 seconds\n",
      "\n",
      "Reading in the Spotify data from the .json...\n",
      "Finished reading in the .json! It took 12.681 seconds\n",
      "\n",
      "Removing songs without both an embedding and audio features...\n",
      "Finished removing the songs! It took 0.134 seconds.\n"
     ]
    }
   ],
   "source": [
    "songTsvPath = \"C:\\\\Data\\\\College\\\\CS 682 - Neural Networks\\\\Project\\\\Task 3 - Network Development\\\\Data\\\\Song Embeddings - 128 dim.tsv\"\n",
    "songJsonPath = \"C:\\\\Data\\\\College\\\\CS 682 - Neural Networks\\\\Project\\\\Task 1 - Data Collection\\\\Data\\\\Genius Info + Spotify Features, 1990-2010.json\"\n",
    "songs = SongDataset(songTsvPath, songJsonPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through each of the songs to check if they're constructed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the songs dataset is 10086\n"
     ]
    }
   ],
   "source": [
    "_, emb, audio_features, _ = songs[0]\n",
    "audioDimCount = len(audio_features)\n",
    "embDimCount = len(emb)\n",
    "print(\"The length of the songs dataset is %d\" % len(songs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the training / validation split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationSplit = .2\n",
    "songAmt = len(songs)\n",
    "splitIdx = int(np.floor(songAmt * validationSplit))\n",
    "indices = list(range(songAmt))\n",
    "np.random.shuffle(indices)\n",
    "trainIndices = indices[splitIdx:]\n",
    "valIndices = indices[:splitIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Samplers and DataLoaders for the train & validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_train = SubsetRandomSampler(trainIndices)\n",
    "sampler_val = SubsetRandomSampler(valIndices)\n",
    "loader_train = torch.utils.data.DataLoader(songs, batch_size=64, sampler=sampler_train)\n",
    "loader_val = torch.utils.data.DataLoader(songs, batch_size=64, sampler=sampler_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the embedding model using nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embHidden1 = 1024\n",
    "embHidden2 = 512\n",
    "embHidden3 = 256\n",
    "embHidden4 = 32\n",
    "embModel = nn.Sequential(nn.Linear(embDimCount, embHidden1),\n",
    "                      nn.ReLU(),\n",
    "                      nn.BatchNorm1d(embHidden1),\n",
    "                      nn.Linear(embHidden1, embHidden2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.BatchNorm1d(embHidden2),\n",
    "                      nn.Linear(embHidden2, embHidden3),\n",
    "                      nn.ReLU(),\n",
    "                      nn.BatchNorm1d(embHidden3),\n",
    "                      nn.Linear(embHidden3, embHidden4),\n",
    "                      nn.ReLU(),\n",
    "                      nn.BatchNorm1d(embHidden4),\n",
    "                      nn.Linear(embHidden4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the audio model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the audio model using nn.Sequential\n",
    "# audioModel = nn.Sequential(nn.Linear(audioDimCount, audioHidden),\n",
    "#                            nn.ReLU(),\n",
    "#                            nn.Linear(audioHidden, 1))\n",
    "audioHidden = 64\n",
    "audioModel = TwoLayerFC(audioDimCount, audioHidden, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some hyperparameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embEpochs = 1000\n",
    "audioEpochs = 1000\n",
    "embLR = 0.00001\n",
    "audioLR = 0.0005\n",
    "\n",
    "embModel = embModel.to(\"cuda\")\n",
    "audioModel = audioModel.to(\"cuda\")\n",
    "embOptimizer = optim.Adam(embModel.parameters(), lr=embLR)\n",
    "audioOptimizer = optim.Adam(audioModel.parameters(), lr=audioLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(embEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "\n",
    "        # Indicate that we're in training mode\n",
    "        embModel.train()\n",
    "        audioModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Perform a training step for the embedding model \n",
    "        embScores = embModel(emb)\n",
    "        embScores = embScores.reshape(hit.shape)\n",
    "        embLoss = loss_fn(embScores, hit)\n",
    "        embOptimizer.zero_grad()\n",
    "        embLoss.backward()\n",
    "        embOptimizer.step()\n",
    "\n",
    "    # Print the accuracy of the embedding model\n",
    "    embValAcc = checkAccuracy(loader_val, embModel, \"emb\")\n",
    "    embTrainAcc = checkAccuracy(loader_train, embModel, \"emb\")\n",
    "    print(\"\\nEMBEDDING MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, embValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\\n\" % (e, embTrainAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the audio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "\n",
    "        # Indicate that we're in training mode\n",
    "        embModel.train()\n",
    "        audioModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # # Perform a training step for the embedding model \n",
    "        # embScores = embModel(emb)\n",
    "        # embScores = embScores.reshape(hit.shape)\n",
    "        # embLoss = loss_fn(embScores, hit)\n",
    "        # embOptimizer.zero_grad()\n",
    "        # embLoss.backward()\n",
    "        # embOptimizer.step()\n",
    "\n",
    "        # Perform a training step for the audio model\n",
    "        print(audio_features)\n",
    "        audioScores = audioModel(audio_features)\n",
    "        audioScores = audioScores.reshape(hit.shape)\n",
    "        print(audioScores)\n",
    "        print(hit)\n",
    "        audioLoss = loss_fn(audioScores, hit)\n",
    "        audioOptimizer.zero_grad()\n",
    "        audioLoss.backward()\n",
    "        audioOptimizer.step()\n",
    "\n",
    "    # # Print the accuracy of the embedding model\n",
    "    # embValAcc = checkAccuracy(loader_val, embModel, \"emb\")\n",
    "    # embTrainAcc = checkAccuracy(loader_train, embModel, \"emb\")\n",
    "    # print(\"\\nEMBEDDING MODEL:\")\n",
    "    # print(\"Epoch %d: %.4f val accuracy\" % (e, embValAcc))\n",
    "    # print(\"Epoch %d: %.4f train accuracy\\n\" % (e, embTrainAcc))\n",
    "\n",
    "    # Print the accuracy of the audio model\n",
    "    audioValAcc = checkAccuracy(loader_val, audioModel, \"audio\")\n",
    "    audioTrainAcc = checkAccuracy(loader_train, audioModel, \"audio\")\n",
    "    print(\"\\nAUDIO MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, audioValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\" % (e, audioTrainAcc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
