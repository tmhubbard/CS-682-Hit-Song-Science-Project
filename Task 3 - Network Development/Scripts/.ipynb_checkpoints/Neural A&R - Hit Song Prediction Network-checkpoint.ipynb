{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Neural-A&R*: Hit Song Prediction using Social Network + High Level Acoustic Data\n",
    "\n",
    "This Jupyter Notebook was written by Trevor Hubbard and Phillip Sifferlin. It performs Hit Song Prediction using two data sources: \n",
    "\n",
    "- [High-level acoustic data](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/) from Spotify\n",
    "- [node2vec](https://snap.stanford.edu/node2vec/) vector-space embeddings of a music industry social network we made\n",
    "\n",
    "More information about the project can be found [in this GitHub repo.](https://github.com/tmhubbard/CS-682-Hit-Song-Science-Project) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some import statements\n",
    "import torch, json, random, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Setting up Pytorch's use of CUDA; if your computer isn't CUDA-enabled, \n",
    "# you could replace \"cuda\" with \"cpu\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the SongDataset Class\n",
    "In the following cell, we're defining SongDataset; this class extends Pytorch's Dataset class, and will be used to more easily load the data from the respective .tsv's / .json's for the neural networks. \n",
    "\n",
    "The SongDataset class will ensure that the hits / non-hits are balanced; we've got around 200,000 songs in the dataset (~5,000 of which are hits), and the SongDataset constructor will randomly select an equal number of hits and non-hits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class will be used to help in the song data! \n",
    "class SongDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # The init method defines how we'll input the data; \n",
    "    def __init__(self, embeddingTsvPath, audioJsonPath):\n",
    "\n",
    "        # This dictionary will hold all of the data\n",
    "        self.songDict = {}\n",
    "\n",
    "        curTime = time.time()\n",
    "        print(\"\\nReading the network embeddings from the .tsv...\")\n",
    "\n",
    "        # Open the tsv and process the data in it\n",
    "        with open(embeddingTsvPath, \"r\", encoding=\"utf-8\") as tsvFile:\n",
    "\n",
    "            # Iterate through each line of the .tsv and store the info\n",
    "            hitCol = 0\n",
    "            idCol = 0\n",
    "            for lineNum, line in enumerate(tsvFile):\n",
    "\n",
    "                line = line.strip()\n",
    "                splitLine = line.split(\"\\t\")\n",
    "\n",
    "                # If we're on the first line, figure out where the \"hit\" column is\n",
    "                if (lineNum == 0): \n",
    "                    for idx, header in enumerate(splitLine):\n",
    "                        if (header == \"songID\"):\n",
    "                            idCol = idx\n",
    "                        if (header == \"hit\"):\n",
    "                            hitCol = idx\n",
    "                    continue\n",
    "\n",
    "                # Add the song to the songDict\n",
    "                songID = int(splitLine[idCol])\n",
    "                curHit = int(splitLine[hitCol])\n",
    "                if (not songID in self.songDict):\n",
    "                    self.songDict[songID] = {\"hit\": curHit, \"embedding\": None, \"audio features\": None}\n",
    "\n",
    "                # Update the song's embedding \n",
    "                self.songDict[songID][\"embedding\"] = torch.tensor([float(x) for x in splitLine[hitCol+1:]], dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "        print(\"Finished reading in the embeddings! It took %.3f seconds\" % (time.time()-curTime))\n",
    "        curTime = time.time()\n",
    "        print(\"\\nReading in the Spotify data from the .json...\")\n",
    "\n",
    "        # Open the audio features JSON and process the data in it\n",
    "        with open(audioJsonPath, \"r\", encoding=\"utf-8\") as jsonFile:\n",
    "            songData = json.load(jsonFile)[\"songs\"]\n",
    "            features = ['duration_ms', 'key', 'mode', 'time_signature', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'valence', 'tempo']\n",
    "            for song in songData:\n",
    "                songID = int(song[\"title\"][1])\n",
    "                curHit = int(song[\"hit\"])\n",
    "                if (song[\"audio_features\"] not in [{}, None]):\n",
    "                    if (songID not in self.songDict):\n",
    "                        self.songDict[songID] = {\"hit\": curHit, \"embedding\": None, \"audio features\": None}\n",
    "                    self.songDict[songID][\"audio features\"] = torch.tensor([song['audio_features'][feature] for feature in features], dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "        print(\"Finished reading in the .json! It took %.3f seconds\" % (time.time()-curTime))\n",
    "\n",
    "        # Remove any songs that don't have both an embedding and audio features\n",
    "        curTime = time.time()\n",
    "        print(\"\\nRemoving songs without both an embedding and audio features...\")\n",
    "        hitCount = 0\n",
    "        idsToRemove = []\n",
    "        for songNum, songID in enumerate(self.songDict.keys()):\n",
    "            song = self.songDict[songID]\n",
    "            if ((song[\"embedding\"] is None) or (song[\"audio features\"] is None)):\n",
    "                idsToRemove.append(songID)\n",
    "                continue\n",
    "            else:\n",
    "                if (song[\"hit\"] == 1): hitCount += 1\n",
    "        for songID in idsToRemove:\n",
    "            del self.songDict[songID]\n",
    "        print(\"Finished removing the songs! It took %.3f seconds.\" % (time.time()-curTime))\n",
    "\n",
    "        # Creating the songList (a list version of the songDict)\n",
    "        shuffledSongDict = list(self.songDict.keys())\n",
    "        np.random.shuffle(shuffledSongDict)\n",
    "        self.songList = []\n",
    "        nonHitCount = 0\n",
    "        for songID in shuffledSongDict:\n",
    "            song = self.songDict[songID]\n",
    "\n",
    "            # Skip if this is a nonHit and we've already added all of those\n",
    "            if (nonHitCount == hitCount and song[\"hit\"] == 0):\n",
    "                continue\n",
    "\n",
    "            self.songList.append(song)\n",
    "            self.songList[-1][\"id\"] = songID\n",
    "\n",
    "            if (song[\"hit\"] == 0):\n",
    "                nonHitCount += 1\n",
    "\n",
    "    # The len method returns the length of x_data\n",
    "    def __len__(self):\n",
    "        return len(self.songList)\n",
    "\n",
    "    # The getitem method will specify how to return a particular index\n",
    "    def __getitem__(self, idx):\n",
    "        if (torch.is_tensor(idx)):\n",
    "            idx = idx.tolist()\n",
    "        song = self.songList[idx]\n",
    "        songID = song[\"id\"]\n",
    "        emb = song[\"embedding\"]\n",
    "        audio_features = song[\"audio features\"]\n",
    "        hit = torch.tensor(song[\"hit\"], dtype=torch.float32, device=\"cuda\")\n",
    "        return (songID, emb, audio_features, hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training / Validation Splits\n",
    "\n",
    "After defining the SongDataset class, we can import the data files (a .json for the acoustic data, and .tsv for the social network embeddings) and create a new SongDataset object from them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're running this on your own computer, replace these paths w/ \n",
    "# a path to wherever you put the data. \n",
    "songTsvPath = \"C:\\\\Data\\\\College\\\\CS 682 - Neural Networks\\\\Project\\\\Task 3 - Network Development\\\\Data\\\\Song Embeddings - 10 dim (average).tsv\"\n",
    "songJsonPath = \"C:\\\\Data\\\\College\\\\CS 682 - Neural Networks\\\\Project\\\\Task 1 - Data Collection\\\\Data\\\\Genius Info + Spotify Features, 1990-2010.json\"\n",
    "songs = SongDataset(songTsvPath, songJsonPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare what percentage of the data will be validation data, and then \n",
    "# split up the songs into training and validation sets\n",
    "validationSplit = .2\n",
    "songAmt = len(songs)\n",
    "splitIdx = int(np.floor(songAmt * validationSplit))\n",
    "indices = list(range(songAmt))\n",
    "np.random.shuffle(indices)\n",
    "trainIndices = indices[splitIdx:]\n",
    "valIndices = indices[:splitIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader objects for the Training and Validation data\n",
    "sampler_train = SubsetRandomSampler(trainIndices)\n",
    "sampler_val = SubsetRandomSampler(valIndices)\n",
    "loader_train = torch.utils.data.DataLoader(songs, batch_size=64, sampler=sampler_train)\n",
    "loader_val = torch.utils.data.DataLoader(songs, batch_size=64, sampler=sampler_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating checkAccuracy( ) \n",
    "\n",
    "The following method, checkAccuracy(), will be used to check the accuracy of a given model. Since we've defined a number of different models throughout the notebook, there are a couple of different ways to handle checking the accuracy. Don't worry about each different model right now - they'll make more sense as you progress through the notebook! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will check the accuracy of the model using data from the loader\n",
    "def checkAccuracy(loader, model, modelType):\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (songID, emb, audio_features, hit) in loader:\n",
    "            binaryScores = []\n",
    "            if (modelType == \"emb\"):\n",
    "                binaryScores = torch.round(torch.sigmoid(model(emb))).reshape(hit.shape)\n",
    "            elif (modelType == \"audio\"):\n",
    "                binaryScores = torch.round(torch.sigmoid(model(audio_features))).reshape(hit.shape)\n",
    "            elif (modelType == \"split\"):\n",
    "                audioScores = audioModel_cutoff(audio_features)\n",
    "                embScores = embModel_cutoff(emb)\n",
    "                inputFeatures = torch.cat((embScores, audioScores), dim=1)\n",
    "                binaryScores = torch.round(torch.sigmoid(model(inputFeatures))).reshape(hit.shape)\n",
    "            elif(modelType == \"concat\"):\n",
    "                inputData = torch.cat((emb, audio_features), dim=1)\n",
    "                binaryScores = torch.round(torch.sigmoid(model(inputData))).reshape(hit.shape)\n",
    "            elif(modelType == \"weight\"):\n",
    "                embScores = embModel(emb).squeeze(1)\n",
    "                audioScores = audioModel(audio_features).squeeze(1)\n",
    "                weights = weightModel(torch.cat((emb, audio_features), dim=1))\n",
    "                weightedScores = (embScores * weights[:, 0]) + (audioScores * weights[:, 1])\n",
    "                binaryScores = torch.round(torch.sigmoid(weightedScores)).reshape(hit.shape)\n",
    "            num_correct += (hit == binaryScores).sum().float()\n",
    "            num_samples += len(hit)\n",
    "        return (num_correct/num_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acoustic Data Model\n",
    "We figured it'd be interesting to train separate models on either set of data before combining them together; that way, we'd be able to see which data source (the social network embeddings, or the high-level acoustic data) contained more useful predictive information. \n",
    "\n",
    "The following cells are associated with setting up the model that's only trained on the acoustic data. This is a 4-layer, fully connected network with a couple of Batch-Normalization layers slipped throughout. We've found that it'll generally achieve ~74% accuracy on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the audio model using nn.Sequential\n",
    "audioDimCount = (len(songs[0][2]))\n",
    "audioHidden1 = 256\n",
    "audioHidden2 = 128\n",
    "audioHidden3 = 32\n",
    "audioModel = nn.Sequential(nn.BatchNorm1d(audioDimCount),\n",
    "                           nn.Linear(audioDimCount, audioHidden1),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.BatchNorm1d(audioHidden1),\n",
    "                           nn.Linear(audioHidden1, audioHidden2),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.BatchNorm1d(audioHidden2),\n",
    "                           nn.Linear(audioHidden2, audioHidden3),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.BatchNorm1d(audioHidden3),\n",
    "                           nn.Linear(audioHidden3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the acoustic model\n",
    "audioEpochs = 10\n",
    "audioLR = 0.0001\n",
    "audioModel = audioModel.to(\"cuda\")\n",
    "audioOptimizer = optim.RMSprop(audioModel.parameters(), lr=audioLR)\n",
    "\n",
    "for e in range(audioEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "\n",
    "        # Indicate that we're in training mode\n",
    "        audioModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Perform a training step for the audio model\n",
    "        audioScores = audioModel(audio_features)\n",
    "        audioScores = audioScores.reshape(hit.shape)\n",
    "        audioLoss = loss_fn(audioScores, hit)\n",
    "        audioOptimizer.zero_grad()\n",
    "        audioLoss.backward()\n",
    "        audioOptimizer.step()\n",
    "\n",
    "    # Print the accuracy of the audio model\n",
    "    audioValAcc = checkAccuracy(loader_val, audioModel, \"audio\")\n",
    "    audioTrainAcc = checkAccuracy(loader_train, audioModel, \"audio\")\n",
    "    print(\"\\nAUDIO MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, audioValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\" % (e, audioTrainAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Embedding Model\n",
    "Below, you'll find a declaration of the Network Embedding model. It's a 5-layer, fully connected network with some Batch-Normalization layers spread throughout. We've found that it'll generally achieve ~85% accuracy on the validation set! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the Network Embedding model\n",
    "embDimCount = (len(songs[0][1]))\n",
    "embHidden1 = 1024\n",
    "embHidden2 = 512\n",
    "embHidden3 = 256\n",
    "embHidden4 = 128\n",
    "embModel = nn.Sequential(nn.Linear(embDimCount, embHidden1),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm1d(embHidden1),\n",
    "                      nn.Linear(embHidden1, embHidden2),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm1d(embHidden2),\n",
    "                      nn.Linear(embHidden2, embHidden3),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm1d(embHidden3),\n",
    "                      nn.Linear(embHidden3, embHidden4),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm1d(embHidden4),\n",
    "                      nn.Linear(embHidden4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Network Embedding model\n",
    "embEpochs = 10\n",
    "embLR = 0.000075\n",
    "embModel = embModel.to(\"cuda\")\n",
    "embOptimizer = optim.Adam(embModel.parameters(), lr=embLR)\n",
    "embValAccList = []\n",
    "embTrainAccList = []\n",
    "\n",
    "for e in range(embEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "        \n",
    "        # Indicate that we're in training mode\n",
    "        embModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Perform a training step for the embedding model \n",
    "        embScores = embModel(emb)\n",
    "        embScores = embScores.reshape(hit.shape)\n",
    "        embLoss = loss_fn(embScores, hit)\n",
    "        embOptimizer.zero_grad()\n",
    "        embLoss.backward()\n",
    "        embOptimizer.step()\n",
    "\n",
    "    # Print the accuracy of the embedding model\n",
    "    embValAcc = checkAccuracy(loader_val, embModel, \"emb\")\n",
    "    embTrainAcc = checkAccuracy(loader_train, embModel, \"emb\")\n",
    "    embValAccList.append(embValAcc)\n",
    "    embTrainAccList.append(embTrainAcc)\n",
    "    print(\"\\nEMBEDDING MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, embValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\\n\" % (e, embTrainAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Mixture #1: Bagging\n",
    "\n",
    "Throughout the rest of this Notebook, we attempted different strategies for combining the acoustic + network data. Our first attempt was a Bagging strategy - we can simply take the average of the scores of the acoustic model and the embedding model, and use this as our new score. (So, if embModel guessed 0.85, and audioModel guessed 0.35, then we'd get an average score of 0.6 - we'd then round this up to 1, indicating a hit!) \n",
    "\n",
    "<img src=\"https://i.imgur.com/a2RczMM.png\" width=900 height=824/>\n",
    "\n",
    "\n",
    "\n",
    "This meta-model seemed to perform about as well as the embedding model; it didn't seem to improve accuracy in any significant way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bagging \"meta-model\" doesn't require any training, since we're just\n",
    "# averaging the output of two pre-trained models: embModel and audioModel. \n",
    "# Store the songIDs for each of the correct songs in bothRightGuesses.\n",
    "bothRightGuesses = []\n",
    "embModel.eval()\n",
    "audioModel.eval()\n",
    "with torch.no_grad():\n",
    "    for (songID, emb, audio_features, hit) in loader_val:\n",
    "        embScores = (torch.sigmoid(embModel(emb))).reshape(hit.shape)\n",
    "        audioScores = (torch.sigmoid(audioModel(audio_features))).reshape(hit.shape)\n",
    "        bothScores = (embScores + audioScores)/2\n",
    "        binaryScores = torch.round(bothScores)\n",
    "        mask = (binaryScores == hit)\n",
    "        bothRightGuesses += ([int(x) for x in (songID[mask])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyse the results of the bagging model, run the cells below; they'll collect the correct guesses of both the embedding and acoustic models, and then compare the sets of correct guesses with the correct guesses of the bagged model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which songs the embedding model\n",
    "embRightGuesses = []\n",
    "embModel.eval()\n",
    "with torch.no_grad():\n",
    "    for (songID, emb, audio_features, hit) in loader_val:\n",
    "        binaryScores = torch.round(torch.sigmoid(embModel(emb))).reshape(hit.shape)\n",
    "        mask = (binaryScores == hit)\n",
    "        embRightGuesses += ([int(x) for x in (songID[mask])])\n",
    "        \n",
    "audioRightGuesses = []\n",
    "audioModel.eval()\n",
    "with torch.no_grad():\n",
    "    for (songID, emb, audio_features, hit) in loader_val:\n",
    "        binaryScores = torch.round(torch.sigmoid(audioModel(audio_features))).reshape(hit.shape)\n",
    "        mask = (binaryScores == hit)\n",
    "        audioRightGuesses += ([int(x) for x in (songID[mask])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embRightGuessesSet = set(embRightGuesses)\n",
    "audioRightGuessesSet = set(audioRightGuesses)\n",
    "bothRightGuessesSet = set(bothRightGuesses)\n",
    "print(\"There were %d songs in the validation set...\" % len(valIndices))\n",
    "print(\"The embedding model got %d songs correct (%.2f%% accuracy)\" % (len(embRightGuessesSet), 100 * (len(embRightGuessesSet)/len(valIndices))))\n",
    "print(\"The audio model got %d songs correct (%.2f%% accuracy)\" % (len(audioRightGuesses), 100 * (len(audioRightGuesses)/len(valIndices))))\n",
    "print(\"The bagged model got %d songs correct (%.2f%% accuracy)\" % (len(bothRightGuesses), 100 * (len(bothRightGuesses)/len(valIndices))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Mixture #2: \"Split\" Network\n",
    "\n",
    "For our next model mixture, we tried to create a \"split\" network - essentially, we train two separate models (embModel_cutoff and audioModel_cutoff), which have the same architecture as the regular embModel and audioModels, except cutoff at the last hidden layers. Then, we concatenate the outputs of these models together, and use that as input data for the third model: splitModel. Then, we backpropagate the loss through both of the networks. \n",
    "\n",
    "<img src=\"https://i.imgur.com/ERZ0H3M.png\" width=800 height=866/>\n",
    "\n",
    "This mixture recieves around 85% accuracy on the validation set, occasionally pushing through to 86%. We've seen it get as high as 87% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropoutStrength = 0.6\n",
    "\n",
    "embDimCount = (len(songs[0][1]))\n",
    "embHidden1 = 1024\n",
    "embHidden2 = 512\n",
    "embHidden3 = 256\n",
    "embCutoffAmt = 128\n",
    "embModel_cutoff = nn.Sequential(nn.Linear(embDimCount, embHidden1),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm1d(embHidden1),\n",
    "                      nn.Linear(embHidden1, embHidden2),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.Linear(embHidden2, embHidden3),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm1d(embHidden3),\n",
    "                      nn.Linear(embHidden3, embCutoffAmt),\n",
    "                      nn.Dropout(dropoutStrength))\n",
    "\n",
    "\n",
    "audioDimCount = (len(songs[0][2]))\n",
    "audioHidden1 = 256 \n",
    "audioHidden2 = 128\n",
    "audioCutoffAmt = 64 \n",
    "audioModel_cutoff = nn.Sequential(nn.BatchNorm1d(audioDimCount),\n",
    "                           nn.Linear(audioDimCount, audioHidden1),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.Linear(audioHidden1, audioHidden2),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.BatchNorm1d(audioHidden2),\n",
    "                           nn.Linear(audioHidden2, audioCutoffAmt),\n",
    "                           nn.Dropout(dropoutStrength))\n",
    "\n",
    "\n",
    "splitInput = audioCutoffAmt+embCutoffAmt\n",
    "splitHidden1 = 1024\n",
    "splitHidden2 = 512 \n",
    "splitHidden3 = 256\n",
    "splitHidden4 = 128\n",
    "splitModel = nn.Sequential(nn.Linear(splitInput, splitHidden1),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.Dropout(dropoutStrength),\n",
    "                           nn.Linear(splitHidden1, splitHidden2),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.Linear(splitHidden2, splitHidden3),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.Dropout(dropoutStrength),\n",
    "                           nn.Linear(splitHidden3, splitHidden4),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.Linear(splitHidden4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splitModel = splitModel.to(\"cuda\")\n",
    "audioModel_cutoff = audioModel_cutoff.to(\"cuda\")\n",
    "embModel_cutoff = embModel_cutoff.to(\"cuda\")\n",
    "\n",
    "splitEpochs = 30\n",
    "# Achieved 87% a couple times\n",
    "# splitLR = 0.000005\n",
    "# audioLR = 0.00005\n",
    "# embLR = 0.00005\n",
    "splitLR = 0.000005\n",
    "audioLR = 0.00005\n",
    "embLR = 0.00005\n",
    "\n",
    "splitOptimizer = optim.Adam(splitModel.parameters(), lr=splitLR)\n",
    "embCutoffOptimizer = optim.Adam(embModel_cutoff.parameters(), lr=embLR)\n",
    "audioCutoffOptimizer = optim.Adam(audioModel_cutoff.parameters(), lr=audioLR)\n",
    "\n",
    "for e in range(splitEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "\n",
    "        # Indicate that we're in training mode\n",
    "        embModel_cutoff.train()\n",
    "        audioModel_cutoff.train()\n",
    "        splitModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Perform a training step for the embedding model \n",
    "        embScores = embModel_cutoff(emb)\n",
    "        \n",
    "        # Perform a training step for the audio model\n",
    "        audioScores = audioModel_cutoff(audio_features)\n",
    "        \n",
    "        # Now, use these scores to perform a training step for the split model \n",
    "        inputScores = torch.cat((embScores, audioScores), dim=1)\n",
    "\n",
    "        splitScores = splitModel(inputScores)\n",
    "        splitScores = splitScores.reshape(hit.shape)\n",
    "        splitLoss = loss_fn(splitScores, hit)\n",
    "        splitOptimizer.zero_grad()\n",
    "        audioCutoffOptimizer.zero_grad()\n",
    "        embCutoffOptimizer.zero_grad()\n",
    "        splitLoss.backward()\n",
    "        splitOptimizer.step()\n",
    "        audioCutoffOptimizer.step()\n",
    "        embCutoffOptimizer.step()\n",
    "        \n",
    "\n",
    "    # Print the accuracy of the embedding model\n",
    "    splitValAcc = checkAccuracy(loader_val, splitModel, \"split\")\n",
    "    splitTrainAcc = checkAccuracy(loader_train, splitModel, \"split\")\n",
    "    print(\"\\nSPLIT MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, splitValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\\n\" % (e, splitTrainAcc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Mixture #3: Data Concatenation\n",
    "\n",
    "Next, we tried to train a model on a mixture of the data by concatenating the acoustic and embedding data together, and then training the model on that. Typically, this model underperformed compared to the previous one; it achieved around 83-84% accuracy on the validation set.\n",
    "\n",
    "<img src=\"https://i.imgur.com/ZQznqxT.png\"  width=800 height=500/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatInput  = audioDimCount + embDimCount\n",
    "concatHidden1 = 1024\n",
    "concatHidden2 = 512\n",
    "concatHidden3 = 256\n",
    "concatHidden4 = 128\n",
    "concatModel = nn.Sequential(nn.BatchNorm1d(concatInput),\n",
    "                            nn.Linear(concatInput, concatHidden1),\n",
    "                            nn.LeakyReLU(),\n",
    "                            #nn.BatchNorm1d(concatHidden1),\n",
    "                            nn.Linear(concatHidden1, concatHidden2),\n",
    "                            nn.LeakyReLU(),\n",
    "                            #nn.BatchNorm1d(concatHidden2),\n",
    "                            nn.Linear(concatHidden2, concatHidden3),\n",
    "                            nn.LeakyReLU(),\n",
    "                            #nn.BatchNorm1d(concatHidden3),\n",
    "                            nn.Linear(concatHidden3, concatHidden4),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Linear(concatHidden4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatEpochs = 10\n",
    "concatLR =0.0001\n",
    "concatModel = concatModel.to(\"cuda\")\n",
    "concatOptimizer = optim.Adam(concatModel.parameters(), lr=concatLR)\n",
    "\n",
    "for e in range(concatEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "        \n",
    "        # Indicate that we're in training mode\n",
    "        concatModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Perform a training step for the embedding model\n",
    "        inputData = torch.cat((emb, audio_features), dim=1)\n",
    "        concatScores = concatModel(inputData)\n",
    "        concatScores = concatScores.reshape(hit.shape)\n",
    "        concatLoss = loss_fn(concatScores, hit)\n",
    "        concatOptimizer.zero_grad()\n",
    "        concatLoss.backward()\n",
    "        concatOptimizer.step()\n",
    "\n",
    "    # Print the accuracy of the embedding model\n",
    "    concatValAcc = checkAccuracy(loader_val, concatModel, \"concat\")\n",
    "    concatTrainAcc = checkAccuracy(loader_train, concatModel, \"concat\")\n",
    "    print(\"\\nEMBEDDING MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, concatValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\\n\" % (e, concatTrainAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Mixture #4: \"WeightNet\"\n",
    "\n",
    "We've called the next model mixing strategy \"WeightNet\". Here, we were trying to use some of the intuition behind the Bagging model - we're going to base our model's prediction on an average of the prediction scores given by the pretrained embModel and audioModel. With our original Bagging model, we tried to simply average the two prediction scores; with WeightNet, though, we want a weighted average that's dependent on particular songs. (Certain songs might be \"easier\" for the audioModel to guess, whereas others might be easier for the embModel.) \n",
    "\n",
    "So, in addition to the pretrained embModel and audioModel, we made weightModel. This model has a similar network architecture to the Data Concatenation model, except it outputs two different scores instead of one. We use these two scores as weights for the predictions that come out of embModel and audioModel; then, we backpropagate the loss from the predictions to the weightModel to tweak the weights for that particular example. \n",
    "\n",
    "<img src=\"https://i.imgur.com/GLkDRbj.png\"/>\n",
    "\n",
    "WeightNet will typically achieve results around 84% - 85% accurate on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightInput   = audioDimCount + embDimCount\n",
    "weightHidden1 = 64\n",
    "weightHidden2 = 32\n",
    "\n",
    "\n",
    "weightModel = nn.Sequential(nn.BatchNorm1d(weightInput),\n",
    "                            nn.Linear(weightInput, weightHidden1),\n",
    "                            nn.ReLU(),\n",
    "                            nn.BatchNorm1d(weightHidden1),\n",
    "                            nn.Linear(weightHidden1, weightHidden2),\n",
    "                            nn.ReLU(),                      \n",
    "                            nn.BatchNorm1d(weightHidden2))\n",
    "\n",
    "weightModel = weightModel.to(\"cuda\")\n",
    "audioModel = audioModel.to(\"cuda\")\n",
    "embModel = embModel.to(\"cuda\")\n",
    "\n",
    "weightEpochs = 40\n",
    "weightLR = 0.001\n",
    "weightOptimizer = optim.Adam(weightModel.parameters(), lr=weightLR)\n",
    "\n",
    "for e in range(weightEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "\n",
    "        # Indicate that we're in training mode\n",
    "        weightModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        embScores = embModel(emb).squeeze(1)        \n",
    "        audioScores = audioModel(audio_features).squeeze(1)\n",
    "\n",
    "        weights = weightModel(torch.cat((emb, audio_features), dim=1))\n",
    "        weightedScores = (embScores * weights[:, 0]) + (audioScores * weights[:, 1])\n",
    "\n",
    "        weightLoss = loss_fn(weightedScores, hit)\n",
    "        weightOptimizer.zero_grad()\n",
    "        weightLoss.backward()\n",
    "        weightOptimizer.step()\n",
    "\n",
    "    # Print the accuracy of the embedding model\n",
    "    weightValAcc = checkAccuracy(loader_val, weightModel, \"weight\")\n",
    "    weightTrainAcc = checkAccuracy(loader_train, weightModel, \"weight\")\n",
    "    print(\"\\nWEIGHT MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, weightValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\\n\" % (e, weightTrainAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis\n",
    "This section of the Notebook is dedicated to doing some performance analysis of various facets of the project! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Embedding Dimensionality\n",
    "When we encode our social network w/ node2vec, one of the hyperparameters we specify is \"dimensionality\" (i.e., how many dimensions each of the node embeddings will have). We've found that the dimensionality of the embeddings makes a decent impact on the performance of the embedding model; as a result, we've included the following loop below, which visualizes the effect of embedding dimensionality on the accuracies of the embedding model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some parameters for this dimensionality visualization\n",
    "dimensionalities = [1, 2, 4, 8, 10, 12, 16, 32, 128]\n",
    "dimLoaders = {} # dimensionality --> (training DataLoader, validation DataLoader)\n",
    "batchSize = 16\n",
    "validationSplit = .2\n",
    "\n",
    "# Iterate through each dimensionality, and create a DataLoader for it \n",
    "for dim in dimensionalities:\n",
    "    \n",
    "    # Make the SongDataset\n",
    "    print(\"\\nCREATING DATALOADER FOR DIM=%d\" % dim)\n",
    "    curDimTsvPath = \"C:\\\\Data\\\\College\\\\CS 682 - Neural Networks\\\\Project\\\\Task 3 - Network Development\\\\Data\\\\Song Embeddings - \" + str(dim) + \" dim (average).tsv\"\n",
    "    curSongs = SongDataset(curDimTsvPath, songJsonPath)\n",
    "    \n",
    "    # Figure out which indices are going into the Dataloader\n",
    "    songAmt = len(songs)\n",
    "    splitIdx = int(np.floor(songAmt * validationSplit))\n",
    "    indices = list(range(songAmt))\n",
    "    np.random.shuffle(indices)\n",
    "    trainIndices = indices[splitIdx:]\n",
    "    valIndices = indices[:splitIdx]\n",
    "    \n",
    "    # Create the DataLoaders\n",
    "    curSampler_train = SubsetRandomSampler(trainIndices)\n",
    "    curSampler_val = SubsetRandomSampler(valIndices)\n",
    "    curLoader_train = torch.utils.data.DataLoader(curSongs, batch_size=batchSize, sampler=curSampler_train)\n",
    "    curLoader_val = torch.utils.data.DataLoader(curSongs, batch_size=batchSize, sampler=curSampler_val)\n",
    "    \n",
    "    # Store the DataLoaders in the dimLoaders dict\n",
    "    dimLoaders[dim] = (curLoader_train, curLoader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we'll need to train embModel on each dimensionality, and store the performance across epochs\n",
    "dimPerformance = {}\n",
    "embEpochs = 10\n",
    "\n",
    "for dim in dimLoaders.keys():\n",
    "    \n",
    "    # Grab the different DataLoaders\n",
    "    trainLoader = dimLoaders[dim][0]\n",
    "    valLoader = dimLoaders[dim][1]\n",
    "   \n",
    "    # Declare the new embModel\n",
    "    embDimCount = dim\n",
    "    embHidden1 = 1024\n",
    "    embHidden2 = 512\n",
    "    embHidden3 = 256\n",
    "    embHidden4 = 128\n",
    "    embModel = nn.Sequential(nn.Linear(embDimCount, embHidden1),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.BatchNorm1d(embHidden1),\n",
    "                          nn.Linear(embHidden1, embHidden2),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.BatchNorm1d(embHidden2),\n",
    "                          nn.Linear(embHidden2, embHidden3),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.BatchNorm1d(embHidden3),\n",
    "                          nn.Linear(embHidden3, embHidden4),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.BatchNorm1d(embHidden4),\n",
    "                          nn.Linear(embHidden4, 1))\n",
    "    \n",
    "    # Declare training parameters\n",
    "    embLR = 0.000001\n",
    "    embModel = embModel.to(\"cuda\")\n",
    "    embOptimizer = optim.Adam(embModel.parameters(), lr=embLR)\n",
    "    embValAccList = []\n",
    "    embTrainAccList = []\n",
    "\n",
    "    # Train the models for \n",
    "    print(\"\\nTraining the embModel on dim %d\" % dim)\n",
    "    for e in range(embEpochs):\n",
    "        for idx, (songID, emb, audio_features, hit) in enumerate(trainLoader):\n",
    "\n",
    "            # Indicate that we're in training mode\n",
    "            embModel.train()\n",
    "\n",
    "            # Declaring a loss function\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "            # Perform a training step for the embedding model \n",
    "            embScores = embModel(emb)\n",
    "            embScores = embScores.reshape(hit.shape)\n",
    "            embLoss = loss_fn(embScores, hit)\n",
    "            embOptimizer.zero_grad()\n",
    "            embLoss.backward()\n",
    "            embOptimizer.step()\n",
    "\n",
    "        # Print the accuracy of the embedding model\n",
    "        embValAcc = checkAccuracy(valLoader, embModel, \"emb\")\n",
    "        embTrainAcc = checkAccuracy(trainLoader, embModel, \"emb\")\n",
    "        embValAccList.append(embValAcc)\n",
    "        embTrainAccList.append(embTrainAcc)\n",
    "        print(\"\\nEMBEDDING MODEL:\")\n",
    "        print(\"Epoch %d: %.4f val accuracy\" % (e, embValAcc))\n",
    "        print(\"Epoch %d: %.4f train accuracy\\n\" % (e, embTrainAcc))\n",
    "    \n",
    "    # Once the model has been trained, store its performance trace in dimPerformance\n",
    "    dimPerformance[dim] = (embTrainAccList, embValAccList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the different plt figures\n",
    "trainFig = plt.figure(figsize=(10, 10))\n",
    "trainAx = trainFig.add_subplot()\n",
    "\n",
    "# Plot the different training accuracies \n",
    "epochCount = list(range(embEpochs))\n",
    "legend = []\n",
    "for dim in dimPerformance.keys():\n",
    "    trainAcc = dimPerformance[dim][0]\n",
    "    trainAx.plot(epochCount, trainAcc)\n",
    "    legend.append(\"dim = %d\" % dim)\n",
    "trainAx.legend(legend, loc='upper left')\n",
    "plt.title(\"Embedding Dimensionality's Impact on Training Accuracy\")\n",
    "plt.xlabel(\"Epoch number\")\n",
    "plt.ylabel(\"Training accuracy\")\n",
    "plt.show()\n",
    "\n",
    "# Set up the different plt figures\n",
    "valFig = plt.figure(figsize=(10, 10))\n",
    "valAx = valFig.add_subplot()\n",
    "\n",
    "# Plot the different training accuracies \n",
    "for dim in dimPerformance.keys():\n",
    "    valAcc = dimPerformance[dim][1]\n",
    "    valAx.plot(epochCount, valAcc)\n",
    "valAx.legend(legend, loc='upper left')\n",
    "plt.title(\"Embedding Dimensionality's Impact on Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch number\")\n",
    "plt.ylabel(\"Validation accuracy\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
