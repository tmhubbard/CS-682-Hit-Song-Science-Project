{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Neural-A&R*: Hit Song Prediction using Social Network + High Level Acoustic Data\n",
    "\n",
    "This Jupyter Notebook was written by Trevor Hubbard and Phillip Sifferlin. It performs Hit Song Prediction using two data sources: \n",
    "\n",
    "- [High-level acoustic data](https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/) from Spotify\n",
    "- Vector-embeddings of a music industry social network \n",
    "\n",
    "More information about the project can be found [in this GitHub repo.](https://github.com/tmhubbard/CS-682-Hit-Song-Science-Project) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some import statements\n",
    "import torch, json, random, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Setting up Pytorch's use of CUDA; if your computer isn't CUDA-enabled, \n",
    "# you could replace \"cuda\" with \"cpu\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the SongDataset Class\n",
    "In the following cell, we're defining SongDataset; this class extends Pytorch's Dataset class, and will be used to more easily load the data from the respective .tsv's / .json's for the neural networks. \n",
    "\n",
    "The SongDataset class will ensure that the hits / non-hits are balanced; we've got around 200,000 songs in the dataset (~5,000 of which are hits), and the SongDataset constructor will randomly select an equal number of hits and non-hits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class will be used to help in the song data! \n",
    "class SongDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # The init method defines how we'll input the data; \n",
    "    def __init__(self, embeddingTsvPath, audioJsonPath):\n",
    "\n",
    "        # This dictionary will hold all of the data\n",
    "        self.songDict = {}\n",
    "\n",
    "        curTime = time.time()\n",
    "        print(\"\\nReading the network embeddings from the .tsv...\")\n",
    "\n",
    "        # Open the tsv and process the data in it\n",
    "        with open(embeddingTsvPath, \"r\", encoding=\"utf-8\") as tsvFile:\n",
    "\n",
    "            # Iterate through each line of the .tsv and store the info\n",
    "            hitCol = 0\n",
    "            idCol = 0\n",
    "            for lineNum, line in enumerate(tsvFile):\n",
    "\n",
    "                line = line.strip()\n",
    "                splitLine = line.split(\"\\t\")\n",
    "\n",
    "                # If we're on the first line, figure out where the \"hit\" column is\n",
    "                if (lineNum == 0): \n",
    "                    for idx, header in enumerate(splitLine):\n",
    "                        if (header == \"songID\"):\n",
    "                            idCol = idx\n",
    "                        if (header == \"hit\"):\n",
    "                            hitCol = idx\n",
    "                    continue\n",
    "\n",
    "                # Add the song to the songDict\n",
    "                songID = int(splitLine[idCol])\n",
    "                curHit = int(splitLine[hitCol])\n",
    "                if (not songID in self.songDict):\n",
    "                    self.songDict[songID] = {\"hit\": curHit, \"embedding\": None, \"audio features\": None}\n",
    "\n",
    "                # Update the song's embedding \n",
    "                self.songDict[songID][\"embedding\"] = torch.tensor([float(x) for x in splitLine[hitCol+1:]], dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "        print(\"Finished reading in the embeddings! It took %.3f seconds\" % (time.time()-curTime))\n",
    "        curTime = time.time()\n",
    "        print(\"\\nReading in the Spotify data from the .json...\")\n",
    "\n",
    "        # Open the audio features JSON and process the data in it\n",
    "        with open(audioJsonPath, \"r\", encoding=\"utf-8\") as jsonFile:\n",
    "            songData = json.load(jsonFile)[\"songs\"]\n",
    "            features = ['duration_ms', 'key', 'mode', 'time_signature', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'valence', 'tempo']\n",
    "            for song in songData:\n",
    "                songID = int(song[\"title\"][1])\n",
    "                curHit = int(song[\"hit\"])\n",
    "                if (song[\"audio_features\"] not in [{}, None]):\n",
    "                    if (songID not in self.songDict):\n",
    "                        self.songDict[songID] = {\"hit\": curHit, \"embedding\": None, \"audio features\": None}\n",
    "                    self.songDict[songID][\"audio features\"] = torch.tensor([song['audio_features'][feature] for feature in features], dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "        print(\"Finished reading in the .json! It took %.3f seconds\" % (time.time()-curTime))\n",
    "\n",
    "        # Remove any songs that don't have both an embedding and audio features\n",
    "        curTime = time.time()\n",
    "        print(\"\\nRemoving songs without both an embedding and audio features...\")\n",
    "        hitCount = 0\n",
    "        idsToRemove = []\n",
    "        for songNum, songID in enumerate(self.songDict.keys()):\n",
    "            song = self.songDict[songID]\n",
    "            if ((song[\"embedding\"] is None) or (song[\"audio features\"] is None)):\n",
    "                idsToRemove.append(songID)\n",
    "                continue\n",
    "            else:\n",
    "                if (song[\"hit\"] == 1): hitCount += 1\n",
    "        for songID in idsToRemove:\n",
    "            del self.songDict[songID]\n",
    "        print(\"Finished removing the songs! It took %.3f seconds.\" % (time.time()-curTime))\n",
    "\n",
    "        # Creating the songList (a list version of the songDict)\n",
    "        shuffledSongDict = list(self.songDict.keys())\n",
    "        np.random.shuffle(shuffledSongDict)\n",
    "        self.songList = []\n",
    "        nonHitCount = 0\n",
    "        for songID in shuffledSongDict:\n",
    "            song = self.songDict[songID]\n",
    "\n",
    "            # Skip if this is a nonHit and we've already added all of those\n",
    "            if (nonHitCount == hitCount and song[\"hit\"] == 0):\n",
    "                continue\n",
    "\n",
    "            self.songList.append(song)\n",
    "            self.songList[-1][\"id\"] = songID\n",
    "\n",
    "            if (song[\"hit\"] == 0):\n",
    "                nonHitCount += 1\n",
    "\n",
    "    # The len method returns the length of x_data\n",
    "    def __len__(self):\n",
    "        return len(self.songList)\n",
    "\n",
    "    # The getitem method will specify how to return a particular index\n",
    "    def __getitem__(self, idx):\n",
    "        if (torch.is_tensor(idx)):\n",
    "            idx = idx.tolist()\n",
    "        song = self.songList[idx]\n",
    "        songID = song[\"id\"]\n",
    "        emb = song[\"embedding\"]\n",
    "        audio_features = song[\"audio features\"]\n",
    "        hit = torch.tensor(song[\"hit\"], dtype=torch.float32, device=\"cuda\")\n",
    "        return (songID, emb, audio_features, hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training / Validation Splits\n",
    "\n",
    "After defining the SongDataset class, we can import the data files (a .json for the acoustic data, and .tsv for the social network embeddings) and create a new SongDataset object from them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading the network embeddings from the .tsv...\n",
      "Finished reading in the embeddings! It took 22.341 seconds\n",
      "\n",
      "Reading in the Spotify data from the .json...\n",
      "Finished reading in the .json! It took 12.948 seconds\n",
      "\n",
      "Removing songs without both an embedding and audio features...\n",
      "Finished removing the songs! It took 0.138 seconds.\n"
     ]
    }
   ],
   "source": [
    "# If you're running this on your own computer, replace these paths w/ \n",
    "# a path to wherever you put the data. \n",
    "songTsvPath = \"C:\\\\Data\\\\College\\\\CS 682 - Neural Networks\\\\Project\\\\Task 3 - Network Development\\\\Data\\\\Song Embeddings - 128 dim (half).tsv\"\n",
    "songJsonPath = \"C:\\\\Data\\\\College\\\\CS 682 - Neural Networks\\\\Project\\\\Task 1 - Data Collection\\\\Data\\\\Genius Info + Spotify Features, 1990-2010.json\"\n",
    "songs = SongDataset(songTsvPath, songJsonPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare what percentage of the data will be validation data, and then \n",
    "# split up the songs into training and validation sets\n",
    "validationSplit = .2\n",
    "songAmt = len(songs)\n",
    "splitIdx = int(np.floor(songAmt * validationSplit))\n",
    "indices = list(range(songAmt))\n",
    "np.random.shuffle(indices)\n",
    "trainIndices = indices[splitIdx:]\n",
    "valIndices = indices[:splitIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader objects for the Training and Validation data\n",
    "sampler_train = SubsetRandomSampler(trainIndices)\n",
    "sampler_val = SubsetRandomSampler(valIndices)\n",
    "loader_train = torch.utils.data.DataLoader(songs, batch_size=64, sampler=sampler_train)\n",
    "loader_val = torch.utils.data.DataLoader(songs, batch_size=64, sampler=sampler_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating checkAccuracy( ) \n",
    "\n",
    "The following method, checkAccuracy(), will be used to check the accuracy of a given model. Since we've defined a number of different models throughout the notebook, there are a couple of different ways to handle checking the accuracy. Don't worry about each different model right now - they'll make more sense as you progress through the notebook! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will check the accuracy of the model using data from the loader\n",
    "def checkAccuracy(loader, model, modelType):\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (songID, emb, audio_features, hit) in loader:\n",
    "            binaryScores = []\n",
    "            if (modelType == \"emb\"):\n",
    "                binaryScores = torch.round(torch.sigmoid(model(emb))).reshape(hit.shape)\n",
    "            elif (modelType == \"audio\"):\n",
    "                binaryScores = torch.round(torch.sigmoid(model(audio_features))).reshape(hit.shape)\n",
    "            elif (modelType == \"split\"):\n",
    "                audioScores = audioModel_cutoff(audio_features)\n",
    "                embScores = embModel_cutoff(emb)\n",
    "                inputFeatures = torch.cat((embScores, audioScores), dim=1)\n",
    "                binaryScores = torch.round(torch.sigmoid(model(inputFeatures))).reshape(hit.shape)\n",
    "            elif(modelType == \"concat\"):\n",
    "                inputData = torch.cat((emb, audio_features), dim=1)\n",
    "                binaryScores = torch.round(torch.sigmoid(model(inputData))).reshape(hit.shape)\n",
    "            elif(modelType == \"weight\"):\n",
    "                embScores = embModel(emb).squeeze(1)\n",
    "                audioScores = audioModel(audio_features).squeeze(1)\n",
    "                weights = weightModel(torch.cat((emb, audio_features), dim=1))\n",
    "                weightedScores = (embScores * weights[:, 0]) + (audioScores * weights[:, 1])\n",
    "                binaryScores = torch.round(torch.sigmoid(weightedScores)).reshape(hit.shape)\n",
    "            num_correct += (hit == binaryScores).sum().float()\n",
    "            num_samples += len(hit)\n",
    "        return (num_correct/num_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acoustic Data Model\n",
    "We figured it'd be interesting to train separate models on either set of data before combining them together; that way, we'd be able to see which data source (the social network embeddings, or the high-level acoustic data) contained more useful predictive information. \n",
    "\n",
    "The following cells are associated with setting up the model that's only trained on the acoustic data. This is a 4-layer, fully connected network with a couple of Batch-Normalization layers slipped throughout. We've found that it'll generally achieve ~74% accuracy on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the audio model using nn.Sequential\n",
    "audioDimCount = (len(songs[0][2]))\n",
    "audioHidden1 = 256\n",
    "audioHidden2 = 128\n",
    "audioHidden3 = 32\n",
    "audioModel = nn.Sequential(nn.BatchNorm1d(audioDimCount),\n",
    "                           nn.Linear(audioDimCount, audioHidden1),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.BatchNorm1d(audioHidden1),\n",
    "                           nn.Linear(audioHidden1, audioHidden2),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.BatchNorm1d(audioHidden2),\n",
    "                           nn.Linear(audioHidden2, audioHidden3),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.BatchNorm1d(audioHidden3),\n",
    "                           nn.Linear(audioHidden3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUDIO MODEL:\n",
      "Epoch 0: 0.7100 val accuracy\n",
      "Epoch 0: 0.7215 train accuracy\n",
      "\n",
      "AUDIO MODEL:\n",
      "Epoch 1: 0.7105 val accuracy\n",
      "Epoch 1: 0.7431 train accuracy\n",
      "\n",
      "AUDIO MODEL:\n",
      "Epoch 2: 0.7199 val accuracy\n",
      "Epoch 2: 0.7421 train accuracy\n",
      "\n",
      "AUDIO MODEL:\n",
      "Epoch 3: 0.7124 val accuracy\n",
      "Epoch 3: 0.7513 train accuracy\n",
      "\n",
      "AUDIO MODEL:\n",
      "Epoch 4: 0.7194 val accuracy\n",
      "Epoch 4: 0.7549 train accuracy\n",
      "\n",
      "AUDIO MODEL:\n",
      "Epoch 5: 0.7199 val accuracy\n",
      "Epoch 5: 0.7544 train accuracy\n",
      "\n",
      "AUDIO MODEL:\n",
      "Epoch 6: 0.7179 val accuracy\n",
      "Epoch 6: 0.7601 train accuracy\n",
      "\n",
      "AUDIO MODEL:\n",
      "Epoch 7: 0.7194 val accuracy\n",
      "Epoch 7: 0.7560 train accuracy\n",
      "\n",
      "AUDIO MODEL:\n",
      "Epoch 8: 0.7219 val accuracy\n",
      "Epoch 8: 0.7633 train accuracy\n",
      "\n",
      "AUDIO MODEL:\n",
      "Epoch 9: 0.7139 val accuracy\n",
      "Epoch 9: 0.7566 train accuracy\n"
     ]
    }
   ],
   "source": [
    "# Train the acoustic model\n",
    "audioEpochs = 10\n",
    "audioLR = 0.0001\n",
    "audioModel = audioModel.to(\"cuda\")\n",
    "audioOptimizer = optim.RMSprop(audioModel.parameters(), lr=audioLR)\n",
    "\n",
    "for e in range(audioEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "\n",
    "        # Indicate that we're in training mode\n",
    "        audioModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Perform a training step for the audio model\n",
    "        audioScores = audioModel(audio_features)\n",
    "        audioScores = audioScores.reshape(hit.shape)\n",
    "        audioLoss = loss_fn(audioScores, hit)\n",
    "        audioOptimizer.zero_grad()\n",
    "        audioLoss.backward()\n",
    "        audioOptimizer.step()\n",
    "\n",
    "    # Print the accuracy of the audio model\n",
    "    audioValAcc = checkAccuracy(loader_val, audioModel, \"audio\")\n",
    "    audioTrainAcc = checkAccuracy(loader_train, audioModel, \"audio\")\n",
    "    print(\"\\nAUDIO MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, audioValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\" % (e, audioTrainAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Embedding Model\n",
    "Below, you'll find a declaration of the Network Embedding model. It's a 5-layer, fully connected network with some Batch-Normalization layers spread throughout. We've found that it'll generally achieve ~85% accuracy on the validation set! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the Network Embedding model\n",
    "embDimCount = (len(songs[0][1]))\n",
    "embHidden1 = 1024\n",
    "embHidden2 = 512\n",
    "embHidden3 = 256\n",
    "embHidden4 = 128\n",
    "embModel = nn.Sequential(nn.Linear(embDimCount, embHidden1),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm1d(embHidden1),\n",
    "                      nn.Linear(embHidden1, embHidden2),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm1d(embHidden2),\n",
    "                      nn.Linear(embHidden2, embHidden3),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm1d(embHidden3),\n",
    "                      nn.Linear(embHidden3, embHidden4),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm1d(embHidden4),\n",
    "                      nn.Linear(embHidden4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 0: 0.8270 val accuracy\n",
      "Epoch 0: 0.8352 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 1: 0.8324 val accuracy\n",
      "Epoch 1: 0.8557 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 2: 0.8369 val accuracy\n",
      "Epoch 2: 0.8693 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 3: 0.8409 val accuracy\n",
      "Epoch 3: 0.8845 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 4: 0.8354 val accuracy\n",
      "Epoch 4: 0.8896 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 5: 0.8409 val accuracy\n",
      "Epoch 5: 0.8968 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 6: 0.8468 val accuracy\n",
      "Epoch 6: 0.9021 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 7: 0.8463 val accuracy\n",
      "Epoch 7: 0.9042 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 8: 0.8428 val accuracy\n",
      "Epoch 8: 0.9129 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 9: 0.8443 val accuracy\n",
      "Epoch 9: 0.9216 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 10: 0.8443 val accuracy\n",
      "Epoch 10: 0.9273 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 11: 0.8463 val accuracy\n",
      "Epoch 11: 0.9305 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 12: 0.8413 val accuracy\n",
      "Epoch 12: 0.9346 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 13: 0.8379 val accuracy\n",
      "Epoch 13: 0.9389 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 14: 0.8374 val accuracy\n",
      "Epoch 14: 0.9393 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 15: 0.8374 val accuracy\n",
      "Epoch 15: 0.9413 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 16: 0.8364 val accuracy\n",
      "Epoch 16: 0.9430 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 17: 0.8374 val accuracy\n",
      "Epoch 17: 0.9424 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 18: 0.8369 val accuracy\n",
      "Epoch 18: 0.9497 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 19: 0.8384 val accuracy\n",
      "Epoch 19: 0.9488 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 20: 0.8349 val accuracy\n",
      "Epoch 20: 0.9506 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 21: 0.8354 val accuracy\n",
      "Epoch 21: 0.9482 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 22: 0.8349 val accuracy\n",
      "Epoch 22: 0.9549 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 23: 0.8359 val accuracy\n",
      "Epoch 23: 0.9551 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 24: 0.8334 val accuracy\n",
      "Epoch 24: 0.9546 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 25: 0.8349 val accuracy\n",
      "Epoch 25: 0.9554 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 26: 0.8339 val accuracy\n",
      "Epoch 26: 0.9569 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 27: 0.8354 val accuracy\n",
      "Epoch 27: 0.9569 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 28: 0.8309 val accuracy\n",
      "Epoch 28: 0.9570 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 29: 0.8384 val accuracy\n",
      "Epoch 29: 0.9586 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 30: 0.8369 val accuracy\n",
      "Epoch 30: 0.9582 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 31: 0.8379 val accuracy\n",
      "Epoch 31: 0.9584 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 32: 0.8280 val accuracy\n",
      "Epoch 32: 0.9642 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 33: 0.8334 val accuracy\n",
      "Epoch 33: 0.9603 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 34: 0.8314 val accuracy\n",
      "Epoch 34: 0.9589 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 35: 0.8280 val accuracy\n",
      "Epoch 35: 0.9623 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 36: 0.8354 val accuracy\n",
      "Epoch 36: 0.9574 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 37: 0.8290 val accuracy\n",
      "Epoch 37: 0.9615 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 38: 0.8304 val accuracy\n",
      "Epoch 38: 0.9598 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 39: 0.8334 val accuracy\n",
      "Epoch 39: 0.9600 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 40: 0.8314 val accuracy\n",
      "Epoch 40: 0.9612 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 41: 0.8299 val accuracy\n",
      "Epoch 41: 0.9622 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 42: 0.8314 val accuracy\n",
      "Epoch 42: 0.9638 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 43: 0.8299 val accuracy\n",
      "Epoch 43: 0.9624 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 44: 0.8359 val accuracy\n",
      "Epoch 44: 0.9629 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 45: 0.8294 val accuracy\n",
      "Epoch 45: 0.9654 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 46: 0.8290 val accuracy\n",
      "Epoch 46: 0.9643 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 47: 0.8255 val accuracy\n",
      "Epoch 47: 0.9642 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 48: 0.8309 val accuracy\n",
      "Epoch 48: 0.9587 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 49: 0.8260 val accuracy\n",
      "Epoch 49: 0.9639 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 50: 0.8275 val accuracy\n",
      "Epoch 50: 0.9616 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 51: 0.8255 val accuracy\n",
      "Epoch 51: 0.9627 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 52: 0.8270 val accuracy\n",
      "Epoch 52: 0.9677 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 53: 0.8270 val accuracy\n",
      "Epoch 53: 0.9668 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 54: 0.8280 val accuracy\n",
      "Epoch 54: 0.9663 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 55: 0.8255 val accuracy\n",
      "Epoch 55: 0.9659 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 56: 0.8245 val accuracy\n",
      "Epoch 56: 0.9664 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 57: 0.8260 val accuracy\n",
      "Epoch 57: 0.9653 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 58: 0.8285 val accuracy\n",
      "Epoch 58: 0.9665 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 59: 0.8280 val accuracy\n",
      "Epoch 59: 0.9660 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 60: 0.8285 val accuracy\n",
      "Epoch 60: 0.9649 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 61: 0.8240 val accuracy\n",
      "Epoch 61: 0.9684 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 62: 0.8304 val accuracy\n",
      "Epoch 62: 0.9689 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 63: 0.8250 val accuracy\n",
      "Epoch 63: 0.9659 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 64: 0.8220 val accuracy\n",
      "Epoch 64: 0.9670 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 65: 0.8255 val accuracy\n",
      "Epoch 65: 0.9694 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 66: 0.8304 val accuracy\n",
      "Epoch 66: 0.9698 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 67: 0.8314 val accuracy\n",
      "Epoch 67: 0.9688 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 68: 0.8260 val accuracy\n",
      "Epoch 68: 0.9680 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 69: 0.8290 val accuracy\n",
      "Epoch 69: 0.9691 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 70: 0.8329 val accuracy\n",
      "Epoch 70: 0.9663 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 71: 0.8294 val accuracy\n",
      "Epoch 71: 0.9686 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 72: 0.8299 val accuracy\n",
      "Epoch 72: 0.9689 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 73: 0.8294 val accuracy\n",
      "Epoch 73: 0.9689 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 74: 0.8260 val accuracy\n",
      "Epoch 74: 0.9674 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 75: 0.8290 val accuracy\n",
      "Epoch 75: 0.9696 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 76: 0.8235 val accuracy\n",
      "Epoch 76: 0.9690 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 77: 0.8275 val accuracy\n",
      "Epoch 77: 0.9691 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 78: 0.8250 val accuracy\n",
      "Epoch 78: 0.9675 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 79: 0.8235 val accuracy\n",
      "Epoch 79: 0.9690 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 80: 0.8225 val accuracy\n",
      "Epoch 80: 0.9686 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 81: 0.8290 val accuracy\n",
      "Epoch 81: 0.9690 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 82: 0.8270 val accuracy\n",
      "Epoch 82: 0.9701 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 83: 0.8299 val accuracy\n",
      "Epoch 83: 0.9700 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 84: 0.8309 val accuracy\n",
      "Epoch 84: 0.9695 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 85: 0.8255 val accuracy\n",
      "Epoch 85: 0.9715 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 86: 0.8299 val accuracy\n",
      "Epoch 86: 0.9706 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 87: 0.8255 val accuracy\n",
      "Epoch 87: 0.9710 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 88: 0.8275 val accuracy\n",
      "Epoch 88: 0.9694 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 89: 0.8299 val accuracy\n",
      "Epoch 89: 0.9700 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 90: 0.8265 val accuracy\n",
      "Epoch 90: 0.9695 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 91: 0.8309 val accuracy\n",
      "Epoch 91: 0.9703 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 92: 0.8245 val accuracy\n",
      "Epoch 92: 0.9704 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 93: 0.8250 val accuracy\n",
      "Epoch 93: 0.9695 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 94: 0.8240 val accuracy\n",
      "Epoch 94: 0.9699 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 95: 0.8280 val accuracy\n",
      "Epoch 95: 0.9694 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 96: 0.8255 val accuracy\n",
      "Epoch 96: 0.9704 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 97: 0.8235 val accuracy\n",
      "Epoch 97: 0.9696 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 98: 0.8240 val accuracy\n",
      "Epoch 98: 0.9680 train accuracy\n",
      "\n",
      "\n",
      "EMBEDDING MODEL:\n",
      "Epoch 99: 0.8270 val accuracy\n",
      "Epoch 99: 0.9709 train accuracy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Network Embedding model\n",
    "embEpochs = 100\n",
    "embLR = 0.000025\n",
    "embModel = embModel.to(\"cuda\")\n",
    "embOptimizer = optim.Adam(embModel.parameters(), lr=embLR)\n",
    "embValAccList = []\n",
    "embTrainAccList = []\n",
    "\n",
    "for e in range(embEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "        \n",
    "        # Indicate that we're in training mode\n",
    "        embModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Perform a training step for the embedding model \n",
    "        embScores = embModel(emb)\n",
    "        embScores = embScores.reshape(hit.shape)\n",
    "        embLoss = loss_fn(embScores, hit)\n",
    "        embOptimizer.zero_grad()\n",
    "        embLoss.backward()\n",
    "        embOptimizer.step()\n",
    "\n",
    "    # Print the accuracy of the embedding model\n",
    "    embValAcc = checkAccuracy(loader_val, embModel, \"emb\")\n",
    "    embTrainAcc = checkAccuracy(loader_train, embModel, \"emb\")\n",
    "    embValAccList.append(embValAcc)\n",
    "    embTrainAccList.append(embTrainAcc)\n",
    "    print(\"\\nEMBEDDING MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, embValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\\n\" % (e, embTrainAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Mixture #1: Bagging\n",
    "\n",
    "Throughout the rest of this Notebook, we attempted different strategies for combining the acoustic + network data. Our first attempt was a Bagging strategy - we can simply take the average of the scores of the acoustic model and the embedding model, and use this as our new score. (So, if embModel guessed 0.85, and audioModel guessed 0.35, then we'd get an average score of 0.6 - we'd then round this up to 1, indicating a hit!) \n",
    "\n",
    "This meta-model seemed to perform about as well as the embedding model; it didn't seem to improve accuracy in any significant way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bagging \"meta-model\" doesn't require any training, since we're just\n",
    "# averaging the output of two pre-trained models: embModel and audioModel. \n",
    "# Store the songIDs for each of the correct songs in bothRightGuesses.\n",
    "bothRightGuesses = []\n",
    "embModel.eval()\n",
    "audioModel.eval()\n",
    "with torch.no_grad():\n",
    "    for (songID, emb, audio_features, hit) in loader_val:\n",
    "        embScores = (torch.sigmoid(embModel(emb))).reshape(hit.shape)\n",
    "        audioScores = (torch.sigmoid(audioModel(audio_features))).reshape(hit.shape)\n",
    "        bothScores = (embScores + audioScores)/2\n",
    "        binaryScores = torch.round(bothScores)\n",
    "        mask = (binaryScores == hit)\n",
    "        bothRightGuesses += ([int(x) for x in (songID[mask])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyse the results of the bagging model, run the cells below; they'll collect the correct guesses of both the embedding and acoustic models, and then compare the sets of correct guesses with the correct guesses of the bagged model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out which songs the embedding model\n",
    "embRightGuesses = []\n",
    "embModel.eval()\n",
    "with torch.no_grad():\n",
    "    for (songID, emb, audio_features, hit) in loader_val:\n",
    "        binaryScores = torch.round(torch.sigmoid(embModel(emb))).reshape(hit.shape)\n",
    "        mask = (binaryScores == hit)\n",
    "        embRightGuesses += ([int(x) for x in (songID[mask])])\n",
    "        \n",
    "audioRightGuesses = []\n",
    "audioModel.eval()\n",
    "with torch.no_grad():\n",
    "    for (songID, emb, audio_features, hit) in loader_val:\n",
    "        binaryScores = torch.round(torch.sigmoid(audioModel(audio_features))).reshape(hit.shape)\n",
    "        mask = (binaryScores == hit)\n",
    "        audioRightGuesses += ([int(x) for x in (songID[mask])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embRightGuessesSet = set(embRightGuesses)\n",
    "audioRightGuessesSet = set(audioRightGuesses)\n",
    "bothRightGuessesSet = set(bothRightGuesses)\n",
    "print(\"There were %d songs in the validation set...\" % len(valIndices))\n",
    "print(\"The embedding model got %d songs correct (%.2f%% accuracy)\" % (len(embRightGuessesSet), 100 * (len(embRightGuessesSet)/len(valIndices))))\n",
    "print(\"The audio model got %d songs correct (%.2f%% accuracy)\" % (len(audioRightGuesses), 100 * (len(audioRightGuesses)/len(valIndices))))\n",
    "print(\"The bagged model got %d songs correct (%.2f%% accuracy)\" % (len(bothRightGuesses), 100 * (len(bothRightGuesses)/len(valIndices))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Mixture #2: \"Split\" Network\n",
    "\n",
    "For our next model mixture, we tried to create a \"split\" network - essentially, we train two separate models (embModel_cutoff and audioModel_cutoff), which have the same architecture as the regular embModel and audioModels, except cutoff at the last hidden layers. Then, we concatenate the outputs of these models together, and use that as input data for the third model: splitModel. Then, we backpropagate the loss through both of the networks. \n",
    "\n",
    "<img src=\"https://i.imgur.com/ERZ0H3M.png\" width=600 height=650/>\n",
    "\n",
    "This mixture recieves around 85% accuracy on the validation set, occasionally pushing through to 86%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropoutStrength = 0.7\n",
    "\n",
    "embDimCount = (len(songs[0][1]))\n",
    "embHidden1 = 1024\n",
    "embHidden2 = 512\n",
    "embHidden3 = 256\n",
    "embHidden4 = 128\n",
    "embModel_cutoff = nn.Sequential(nn.Linear(embDimCount, embHidden1),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm1d(embHidden1),\n",
    "                      nn.Linear(embHidden1, embHidden2),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.Linear(embHidden2, embHidden3),\n",
    "                      nn.LeakyReLU(),\n",
    "                      nn.BatchNorm1d(embHidden3),\n",
    "                      nn.Linear(embHidden3, embHidden4),\n",
    "                      nn.Dropout(dropoutStrength))\n",
    "\n",
    "\n",
    "audioDimCount = (len(songs[0][2]))\n",
    "audioHidden1 = 256 \n",
    "audioHidden2 = 128\n",
    "audioHidden3 = 64 \n",
    "audioModel_cutoff = nn.Sequential(nn.BatchNorm1d(audioDimCount),\n",
    "                           nn.Linear(audioDimCount, audioHidden1),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.Linear(audioHidden1, audioHidden2),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.BatchNorm1d(audioHidden2),\n",
    "                           nn.Linear(audioHidden2, audioHidden3),\n",
    "                           nn.Dropout(dropoutStrength))\n",
    "\n",
    "\n",
    "splitInput = embHidden4+audioHidden3\n",
    "splitHidden1 = 1024\n",
    "splitHidden2 = 512\n",
    "splitHidden3 = 256\n",
    "splitHidden4 = 128\n",
    "splitModel = nn.Sequential(nn.Linear(splitInput, splitHidden1),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.Dropout(dropoutStrength),\n",
    "                           nn.Linear(splitHidden1, splitHidden2),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.Linear(splitHidden2, splitHidden3),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.Dropout(dropoutStrength),\n",
    "                           nn.Linear(splitHidden3, splitHidden4),\n",
    "                           nn.LeakyReLU(),\n",
    "                           nn.Linear(splitHidden4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splitModel = splitModel.to(\"cuda\")\n",
    "audioModel_cutoff = audioModel_cutoff.to(\"cuda\")\n",
    "embModel_cutoff = embModel_cutoff.to(\"cuda\")\n",
    "\n",
    "splitEpochs = 20\n",
    "splitLR = 0.000005\n",
    "audioLR = 0.00007\n",
    "embLR = 0.00007\n",
    "splitOptimizer = optim.Adam(splitModel.parameters(), lr=splitLR)\n",
    "embCutoffOptimizer = optim.Adam(audioModel_cutoff.parameters(), lr=embLR)\n",
    "audioCutoffOptimizer = optim.Adam(embModel_cutoff.parameters(), lr=audioLR)\n",
    "\n",
    "for e in range(splitEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "\n",
    "        # Indicate that we're in training mode\n",
    "        embModel_cutoff.train()\n",
    "        audioModel_cutoff.train()\n",
    "        splitModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Perform a training step for the embedding model \n",
    "        embScores = embModel_cutoff(emb)\n",
    "        \n",
    "        # Perform a training step for the audio model\n",
    "        audioScores = audioModel_cutoff(audio_features)\n",
    "        \n",
    "        # Now, use these scores to perform a training step for the split model \n",
    "        inputScores = torch.cat((embScores, audioScores), dim=1)\n",
    "\n",
    "        splitScores = splitModel(inputScores)\n",
    "        splitScores = splitScores.reshape(hit.shape)\n",
    "        splitLoss = loss_fn(splitScores, hit)\n",
    "        splitOptimizer.zero_grad()\n",
    "        audioCutoffOptimizer.zero_grad()\n",
    "        embCutoffOptimizer.zero_grad()\n",
    "        splitLoss.backward()\n",
    "        splitOptimizer.step()\n",
    "        audioCutoffOptimizer.step()\n",
    "        embCutoffOptimizer.step()\n",
    "        \n",
    "\n",
    "    # Print the accuracy of the embedding model\n",
    "    splitValAcc = checkAccuracy(loader_val, splitModel, \"split\")\n",
    "    splitTrainAcc = checkAccuracy(loader_train, splitModel, \"split\")\n",
    "    print(\"\\nSPLIT MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, splitValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\\n\" % (e, splitTrainAcc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Mixture #3: Data Concatenation\n",
    "\n",
    "Next, we tried to train a model on a mixture of the data by concatenating the acoustic and embedding data together, and then training the model on that. Typically, this model underperformed compared to the previous one; it achieved around 83-84% accuracy on the validation set.\n",
    "\n",
    "<img src=\"https://i.imgur.com/ZQznqxT.png\"  width=800 height=500/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatInput  = audioDimCount + embDimCount\n",
    "concatHidden1 = 1024\n",
    "concatHidden2 = 512\n",
    "concatHidden3 = 256\n",
    "concatHidden4 = 128\n",
    "concatModel = nn.Sequential(nn.BatchNorm1d(concatInput),\n",
    "                            nn.Linear(concatInput, concatHidden1),\n",
    "                            nn.LeakyReLU(),\n",
    "                            #nn.BatchNorm1d(concatHidden1),\n",
    "                            nn.Linear(concatHidden1, concatHidden2),\n",
    "                            nn.LeakyReLU(),\n",
    "                            #nn.BatchNorm1d(concatHidden2),\n",
    "                            nn.Linear(concatHidden2, concatHidden3),\n",
    "                            nn.LeakyReLU(),\n",
    "                            #nn.BatchNorm1d(concatHidden3),\n",
    "                            nn.Linear(concatHidden3, concatHidden4),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Linear(concatHidden4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatEpochs = 10\n",
    "concatLR =0.0001\n",
    "concatModel = concatModel.to(\"cuda\")\n",
    "concatOptimizer = optim.Adam(concatModel.parameters(), lr=concatLR)\n",
    "\n",
    "for e in range(concatEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "        \n",
    "        # Indicate that we're in training mode\n",
    "        concatModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Perform a training step for the embedding model\n",
    "        inputData = torch.cat((emb, audio_features), dim=1)\n",
    "        concatScores = concatModel(inputData)\n",
    "        concatScores = concatScores.reshape(hit.shape)\n",
    "        concatLoss = loss_fn(concatScores, hit)\n",
    "        concatOptimizer.zero_grad()\n",
    "        concatLoss.backward()\n",
    "        concatOptimizer.step()\n",
    "\n",
    "    # Print the accuracy of the embedding model\n",
    "    concatValAcc = checkAccuracy(loader_val, concatModel, \"concat\")\n",
    "    concatTrainAcc = checkAccuracy(loader_train, concatModel, \"concat\")\n",
    "    print(\"\\nEMBEDDING MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, concatValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\\n\" % (e, concatTrainAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Mixture #4: \"WeightNet\"\n",
    "da da da "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightInput   = audioDimCount + embDimCount\n",
    "weightHidden1 = 64\n",
    "weightHidden2 = 32\n",
    "\n",
    "\n",
    "weightModel = nn.Sequential(nn.BatchNorm1d(weightInput),\n",
    "                            nn.Linear(weightInput, weightHidden1),\n",
    "                            nn.ReLU(),\n",
    "                            nn.BatchNorm1d(weightHidden1),\n",
    "                            nn.Linear(weightHidden1, weightHidden2),\n",
    "                            nn.ReLU(),                      \n",
    "                            nn.BatchNorm1d(weightHidden2))\n",
    "\n",
    "weightModel = weightModel.to(\"cuda\")\n",
    "audioModel = audioModel.to(\"cuda\")\n",
    "embModel = embModel.to(\"cuda\")\n",
    "\n",
    "weightEpochs = 40\n",
    "weightLR = 0.001\n",
    "weightOptimizer = optim.Adam(weightModel.parameters(), lr=weightLR)\n",
    "\n",
    "for e in range(weightEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "\n",
    "        # Indicate that we're in training mode\n",
    "        weightModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        embScores = embModel(emb).squeeze(1)        \n",
    "        audioScores = audioModel(audio_features).squeeze(1)\n",
    "\n",
    "        weights = weightModel(torch.cat((emb, audio_features), dim=1))\n",
    "        weightedScores = (embScores * weights[:, 0]) + (audioScores * weights[:, 1])\n",
    "\n",
    "        weightLoss = loss_fn(weightedScores, hit)\n",
    "        weightOptimizer.zero_grad()\n",
    "        weightLoss.backward()\n",
    "        weightOptimizer.step()\n",
    "\n",
    "    # Print the accuracy of the embedding model\n",
    "    weightValAcc = checkAccuracy(loader_val, weightModel, \"weight\")\n",
    "    weightTrainAcc = checkAccuracy(loader_train, weightModel, \"weight\")\n",
    "    print(\"\\nWEIGHT MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, weightValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\\n\" % (e, weightTrainAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
