{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This neural network was written by Trevor Hubbard and Phil Sifferlin. It performs Hit Song Prediction using vector embeddings of a music-industry co-collaboration network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some import statements\n",
    "import torch, json, random, time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Setting up Pytorch's use of CUDA \n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASSES**\n",
    "\n",
    "*The next couple cells define the classes we'll use throughout this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class will be used to help in the song data! \n",
    "class SongDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # The init method defines how we'll input the data; \n",
    "    def __init__(self, embeddingTsvPath, audioJsonPath):\n",
    "\n",
    "        # This dictionary will hold all of the data\n",
    "        self.songDict = {}\n",
    "\n",
    "        curTime = time.time()\n",
    "        print(\"\\nReading the network embeddings from the .tsv...\")\n",
    "\n",
    "        # Open the tsv and process the data in it\n",
    "        with open(embeddingTsvPath, \"r\", encoding=\"utf-8\") as tsvFile:\n",
    "\n",
    "            # Iterate through each line of the .tsv and store the info\n",
    "            hitCol = 0\n",
    "            idCol = 0\n",
    "            for lineNum, line in enumerate(tsvFile):\n",
    "\n",
    "                line = line.strip()\n",
    "                splitLine = line.split(\"\\t\")\n",
    "\n",
    "                # If we're on the first line, figure out where the \"hit\" column is\n",
    "                if (lineNum == 0): \n",
    "                    for idx, header in enumerate(splitLine):\n",
    "                        if (header == \"songID\"):\n",
    "                            idCol = idx\n",
    "                        if (header == \"hit\"):\n",
    "                            hitCol = idx\n",
    "                    continue\n",
    "\n",
    "                # Add the song to the songDict\n",
    "                songID = int(splitLine[idCol])\n",
    "                curHit = int(splitLine[hitCol])\n",
    "                if (not songID in self.songDict):\n",
    "                    self.songDict[songID] = {\"hit\": curHit, \"embedding\": None, \"audio features\": None}\n",
    "\n",
    "                # Update the song's embedding \n",
    "                self.songDict[songID][\"embedding\"] = torch.tensor([float(x) for x in splitLine[hitCol+1:]], dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "        print(\"Finished reading in the embeddings! It took %.3f seconds\" % (time.time()-curTime))\n",
    "        curTime = time.time()\n",
    "        print(\"\\nReading in the Spotify data from the .json...\")\n",
    "\n",
    "        # Open the audio features JSON and process the data in it\n",
    "        with open(audioJsonPath, \"r\", encoding=\"utf-8\") as jsonFile:\n",
    "            songData = json.load(jsonFile)[\"songs\"]\n",
    "            features = ['duration_ms', 'key', 'mode', 'time_signature', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'valence', 'tempo']\n",
    "            for song in songData:\n",
    "                songID = int(song[\"title\"][1])\n",
    "                curHit = int(song[\"hit\"])\n",
    "                if (song[\"audio_features\"] not in [{}, None]):\n",
    "                    if (songID not in self.songDict):\n",
    "                        self.songDict[songID] = {\"hit\": curHit, \"embedding\": None, \"audio features\": None}\n",
    "                    self.songDict[songID][\"audio features\"] = torch.tensor([song['audio_features'][feature] for feature in features], dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "        print(\"Finished reading in the .json! It took %.3f seconds\" % (time.time()-curTime))\n",
    "\n",
    "        # Remove any songs that don't have both an embedding and audio features\n",
    "        curTime = time.time()\n",
    "        print(\"\\nRemoving songs without both an embedding and audio features...\")\n",
    "        hitCount = 0\n",
    "        idsToRemove = []\n",
    "        for songNum, songID in enumerate(self.songDict.keys()):\n",
    "            song = self.songDict[songID]\n",
    "            if ((song[\"embedding\"] is None) or (song[\"audio features\"] is None)):\n",
    "                idsToRemove.append(songID)\n",
    "                continue\n",
    "            else:\n",
    "                if (song[\"hit\"] == 1): hitCount += 1\n",
    "        for songID in idsToRemove:\n",
    "            del self.songDict[songID]\n",
    "        print(\"Finished removing the songs! It took %.3f seconds.\" % (time.time()-curTime))\n",
    "\n",
    "        # Creating the songList (a list version of the songDict)\n",
    "        shuffledSongDict = list(self.songDict.keys())\n",
    "        np.random.shuffle(shuffledSongDict)\n",
    "        self.songList = []\n",
    "        nonHitCount = 0\n",
    "        for songID in shuffledSongDict:\n",
    "            song = self.songDict[songID]\n",
    "\n",
    "            # Skip if this is a nonHit and we've already added all of those\n",
    "            if (nonHitCount == hitCount and song[\"hit\"] == 0):\n",
    "                continue\n",
    "\n",
    "            self.songList.append(song)\n",
    "            self.songList[-1][\"id\"] = songID\n",
    "\n",
    "            if (song[\"hit\"] == 0):\n",
    "                nonHitCount += 1\n",
    "\n",
    "    # The len method returns the length of x_data\n",
    "    def __len__(self):\n",
    "        return len(self.songList)\n",
    "\n",
    "    # The getitem method will specify how to return a particular index\n",
    "    def __getitem__(self, idx):\n",
    "        if (torch.is_tensor(idx)):\n",
    "            idx = idx.tolist()\n",
    "        song = self.songList[idx]\n",
    "        songID = song[\"id\"]\n",
    "        emb = song[\"embedding\"]\n",
    "        audio_features = song[\"audio features\"]\n",
    "        hit = torch.tensor(song[\"hit\"], dtype=torch.float32, device=\"cuda\")\n",
    "        return (songID, emb, audio_features, hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is the Two Layer Net that we use for the audio features model**CLASSES**\n",
    "class TwoLayerFC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        scores = self.fc2(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHODS**\n",
    "\n",
    "*The next couple cells define the methods we'll use throughout this notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Dataset from the song embedding .tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading the network embeddings from the .tsv...\n",
      "Finished reading in the embeddings! It took 20.689 seconds\n",
      "\n",
      "Reading in the Spotify data from the .json...\n",
      "Finished reading in the .json! It took 12.087 seconds\n",
      "\n",
      "Removing songs without both an embedding and audio features...\n",
      "Finished removing the songs! It took 0.129 seconds.\n"
     ]
    }
   ],
   "source": [
    "songTsvPath = \"C:\\\\Data\\\\College\\\\CS 682 - Neural Networks\\\\Project\\\\Task 3 - Network Development\\\\Data\\\\Song Embeddings - 128 dim.tsv\"\n",
    "songJsonPath = \"C:\\\\Data\\\\College\\\\CS 682 - Neural Networks\\\\Project\\\\Task 1 - Data Collection\\\\Data\\\\Genius Info + Spotify Features, 1990-2010.json\"\n",
    "songs = SongDataset(songTsvPath, songJsonPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating through each of the songs to check if they're constructed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the songs dataset is 10086\n"
     ]
    }
   ],
   "source": [
    "_, emb, audio_features, _ = songs[0]\n",
    "audioDimCount = len(audio_features)\n",
    "embDimCount = len(emb)\n",
    "print(\"The length of the songs dataset is %d\" % len(songs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the training / validation split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationSplit = .2\n",
    "songAmt = len(songs)\n",
    "splitIdx = int(np.floor(songAmt * validationSplit))\n",
    "indices = list(range(songAmt))\n",
    "np.random.shuffle(indices)\n",
    "trainIndices = indices[splitIdx:]\n",
    "valIndices = indices[:splitIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Samplers and DataLoaders for the train & validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_train = SubsetRandomSampler(trainIndices)\n",
    "sampler_val = SubsetRandomSampler(valIndices)\n",
    "loader_train = torch.utils.data.DataLoader(songs, batch_size=64, sampler=sampler_train)\n",
    "loader_val = torch.utils.data.DataLoader(songs, batch_size=64, sampler=sampler_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the embedding model using nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embHidden1 = 1024\n",
    "embHidden2 = 512\n",
    "embHidden3 = 256\n",
    "embHidden4 = 32\n",
    "embModel = nn.Sequential(nn.Linear(embDimCount, embHidden1),\n",
    "                      nn.ReLU(),\n",
    "                      nn.BatchNorm1d(embHidden1),\n",
    "                      nn.Linear(embHidden1, embHidden2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.BatchNorm1d(embHidden2),\n",
    "                      nn.Linear(embHidden2, embHidden3),\n",
    "                      nn.ReLU(),\n",
    "                      nn.BatchNorm1d(embHidden3),\n",
    "                      nn.Linear(embHidden3, embHidden4),\n",
    "                      nn.ReLU(),\n",
    "                      nn.BatchNorm1d(embHidden4),\n",
    "                      nn.Linear(embHidden4, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the audio model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the audio model using nn.Sequential\n",
    "audioHidden1 = 32\n",
    "audioHidden2 = 16\n",
    "audioHidden3 = 8\n",
    "audioModel = nn.Sequential(nn.BatchNorm1d(audioDimCount),\n",
    "                           nn.Linear(audioDimCount, audioHidden1),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm1d(audioHidden1),\n",
    "                           nn.Linear(audioHidden1, audioHidden2),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm1d(audioHidden2),\n",
    "                           nn.Linear(audioHidden2, audioHidden3),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm1d(audioHidden3),\n",
    "                           nn.Linear(audioHidden3, 1))\n",
    "\n",
    "# audioModel = TwoLayerFC(audioDimCount, audioHidden, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3998 hits in the training data. (out of 8128 songs)\n",
      "There are 1045 hits in the training data. (out of 2048 songs)\n"
     ]
    }
   ],
   "source": [
    "# Checking the makeup of the training and validation data\n",
    "hitCount = 0\n",
    "for (songID, emb, audio_features, hit) in loader_train:\n",
    "    hitCount += len([x for x in list(hit) if x==1])\n",
    "print(\"There are %d hits in the training data. (out of %d songs)\" % (hitCount, len(loader_train)*64))\n",
    "\n",
    "hitCount = 0\n",
    "for (songID, emb, audio_features, hit) in loader_val:\n",
    "    hitCount += len([x for x in list(hit) if x==1])\n",
    "print(\"There are %d hits in the training data. (out of %d songs)\" % (hitCount, len(loader_val)*64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embEpochs = 1000\n",
    "embLR = 0.00001\n",
    "embModel = embModel.to(\"cuda\")\n",
    "embOptimizer = optim.Adam(embModel.parameters(), lr=embLR)\n",
    "\n",
    "for e in range(embEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "\n",
    "        # Indicate that we're in training mode\n",
    "        embModel.train()\n",
    "        audioModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Perform a training step for the embedding model \n",
    "        embScores = embModel(emb)\n",
    "        embScores = embScores.reshape(hit.shape)\n",
    "        embLoss = loss_fn(embScores, hit)\n",
    "        embOptimizer.zero_grad()\n",
    "        embLoss.backward()\n",
    "        embOptimizer.step()\n",
    "\n",
    "    # Print the accuracy of the embedding model\n",
    "    embValAcc = checkAccuracy(loader_val, embModel, \"emb\")\n",
    "    embTrainAcc = checkAccuracy(loader_train, embModel, \"emb\")\n",
    "    print(\"\\nEMBEDDING MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, embValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\\n\" % (e, embTrainAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the audio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audioEpochs = 1000\n",
    "audioLR = 0.0005\n",
    "audioModel = audioModel.to(\"cuda\")\n",
    "audioOptimizer = optim.RMSprop(audioModel.parameters(), lr=audioLR)\n",
    "\n",
    "for e in range(audioEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "\n",
    "        # Indicate that we're in training mode\n",
    "        audioModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Perform a training step for the audio model\n",
    "        audioScores = audioModel(audio_features)\n",
    "        audioScores = audioScores.reshape(hit.shape)\n",
    "        audioLoss = loss_fn(audioScores, hit)\n",
    "        audioOptimizer.zero_grad()\n",
    "        audioLoss.backward()\n",
    "        audioOptimizer.step()\n",
    "\n",
    "    # Print the accuracy of the audio model\n",
    "    audioValAcc = checkAccuracy(loader_val, audioModel, \"audio\")\n",
    "    audioTrainAcc = checkAccuracy(loader_train, audioModel, \"audio\")\n",
    "    print(\"\\nAUDIO MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, audioValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\" % (e, audioTrainAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new, \"split\" network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embHidden1 = 2048\n",
    "embHidden2 = 1024\n",
    "embHidden3 = 512\n",
    "embHidden4 = 256\n",
    "embHidden5 = 128\n",
    "embModel_cutoff = nn.Sequential(nn.Linear(embDimCount, embHidden1),\n",
    "                      nn.ReLU(),\n",
    "                      # nn.BatchNorm1d(embHidden1),\n",
    "                      nn.Linear(embHidden1, embHidden2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.BatchNorm1d(embHidden2),     \n",
    "                      nn.Linear(embHidden2, embHidden3),\n",
    "                      nn.ReLU(),\n",
    "                      # nn.BatchNorm1d(embHidden3),\n",
    "                      nn.Linear(embHidden3, embHidden4),\n",
    "                      nn.ReLU(),\n",
    "                      # nn.BatchNorm1d(embHidden4),\n",
    "                      nn.Linear(embHidden4, embHidden5))\n",
    "\n",
    "\n",
    "audioHidden1 = 1024\n",
    "audioHidden2 = 512\n",
    "audioHidden3 = 256\n",
    "audioHidden4 = 128\n",
    "audioModel_cutoff = nn.Sequential(nn.BatchNorm1d(audioDimCount),\n",
    "                           nn.Linear(audioDimCount, audioHidden1),\n",
    "                           nn.ReLU(),\n",
    "                           # nn.BatchNorm1d(audioHidden1),\n",
    "                           nn.Linear(audioHidden1, audioHidden2),\n",
    "                           nn.ReLU(),\n",
    "                           # nn.BatchNorm1d(audioHidden2),\n",
    "                           nn.Linear(audioHidden2, audioHidden3),\n",
    "                           nn.ReLU(),\n",
    "                           # nn.BatchNorm1d(audioHidden3),\n",
    "                           nn.Linear(audioHidden3, audioHidden4))\n",
    "\n",
    "\n",
    "splitInput = embHidden5+audioHidden4\n",
    "splitHidden1 = 1024\n",
    "splitHidden2 = 512\n",
    "splitHidden3 = 256\n",
    "splitHidden4 = 128\n",
    "splitHidden5 = 64\n",
    "splitModel = nn.Sequential(nn.BatchNorm1d(splitInput),\n",
    "                           nn.Linear(splitInput, splitHidden1),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm1d(splitHidden1),\n",
    "                           nn.Dropout(0.1),\n",
    "                           nn.Linear(splitHidden1, splitHidden2),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm1d(splitHidden2),\n",
    "                           nn.Linear(splitHidden2, splitHidden3),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm1d(splitHidden3),\n",
    "                           nn.Linear(splitHidden3, splitHidden4),\n",
    "                           nn.ReLU(),\n",
    "                           nn.BatchNorm1d(splitHidden4),\n",
    "                           nn.Linear(splitHidden4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitModel = splitModel.to(\"cuda\")\n",
    "audioModel_cutoff = audioModel_cutoff.to(\"cuda\")\n",
    "embModel_cutoff = embModel_cutoff.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method will check the accuracy of the model using data from the loader\n",
    "def checkAccuracy(loader, model, modelType):\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (songID, emb, audio_features, hit) in loader:\n",
    "            binaryScores = []\n",
    "            if (modelType == \"emb\"):\n",
    "                binaryScores = torch.round(torch.sigmoid(model(emb))).reshape(hit.shape)\n",
    "            elif (modelType == \"audio\"):\n",
    "                binaryScores = torch.round(torch.sigmoid(model(audio_features))).reshape(hit.shape)\n",
    "            elif (modelType == \"split\"):\n",
    "                audioScores = audioModel_cutoff(audio_features)\n",
    "                embScores = embModel_cutoff(emb)\n",
    "                inputFeatures = torch.cat((embScores, audioScores), dim=1)\n",
    "                binaryScores = torch.round(torch.sigmoid(model(inputFeatures))).reshape(hit.shape)\n",
    "            num_correct += (hit == binaryScores).sum().float()\n",
    "            num_samples += len(hit)\n",
    "        return (num_correct/num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 0: 0.8483 val accuracy\n",
      "Epoch 0: 0.9372 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 1: 0.8185 val accuracy\n",
      "Epoch 1: 0.9199 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 2: 0.8349 val accuracy\n",
      "Epoch 2: 0.9289 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 3: 0.8413 val accuracy\n",
      "Epoch 3: 0.9331 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 4: 0.8433 val accuracy\n",
      "Epoch 4: 0.9395 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 5: 0.8339 val accuracy\n",
      "Epoch 5: 0.9409 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 6: 0.8433 val accuracy\n",
      "Epoch 6: 0.9482 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 7: 0.8354 val accuracy\n",
      "Epoch 7: 0.9554 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 8: 0.8409 val accuracy\n",
      "Epoch 8: 0.9586 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 9: 0.8409 val accuracy\n",
      "Epoch 9: 0.9533 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 10: 0.8369 val accuracy\n",
      "Epoch 10: 0.9559 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 11: 0.8354 val accuracy\n",
      "Epoch 11: 0.9606 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 12: 0.8354 val accuracy\n",
      "Epoch 12: 0.9581 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 13: 0.8344 val accuracy\n",
      "Epoch 13: 0.9580 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 14: 0.8384 val accuracy\n",
      "Epoch 14: 0.9600 train accuracy\n",
      "\n",
      "\n",
      "SPLIT MODEL:\n",
      "Epoch 15: 0.8339 val accuracy\n",
      "Epoch 15: 0.9601 train accuracy\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-d0578f2f3004>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# Print the accuracy of the embedding model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0msplitValAcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0msplitTrainAcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nSPLIT MODEL:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch %d: %.4f val accuracy\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitValAcc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-150-f84b5225921a>\u001b[0m in \u001b[0;36mcheckAccuracy\u001b[1;34m(loader, model, modelType)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msongID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhit\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mbinaryScores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodelType\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"emb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\CS-682\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\CS-682\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\CS-682\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\CS-682\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-c2d5302f758f>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msong\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"embedding\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0maudio_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msong\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"audio features\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mhit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"hit\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msongID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "splitEpochs = 1000\n",
    "splitLR = 0.00001\n",
    "audioLR = 0.00001\n",
    "embLR = 0.00001\n",
    "splitOptimizer = optim.Adam(splitModel.parameters(), lr=splitLR)\n",
    "embCutoffOptimizer = optim.Adam(audioModel_cutoff.parameters(), lr=embLR)\n",
    "audioCutoffOptimizer = optim.Adam(embModel_cutoff.parameters(), lr=audioLR)\n",
    "\n",
    "for e in range(splitEpochs):\n",
    "    for idx, (songID, emb, audio_features, hit) in enumerate(loader_train):\n",
    "\n",
    "        # Indicate that we're in training mode\n",
    "        embModel_cutoff.train()\n",
    "        audioModel_cutoff.train()\n",
    "        splitModel.train()\n",
    "\n",
    "        # Declaring a loss function\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        # Perform a training step for the embedding model \n",
    "        embScores = embModel_cutoff(emb)\n",
    "        \n",
    "        # Perform a training step for the audio model\n",
    "        audioScores = audioModel_cutoff(audio_features)\n",
    "        \n",
    "        # Now, use these scores to perform a training step for the split model \n",
    "        inputScores = torch.cat((embScores, audioScores), dim=1)\n",
    "\n",
    "        splitScores = splitModel(inputScores)\n",
    "        splitScores = splitScores.reshape(hit.shape)\n",
    "        splitLoss = loss_fn(splitScores, hit)\n",
    "        splitOptimizer.zero_grad()\n",
    "        splitLoss.backward()\n",
    "        splitOptimizer.step()\n",
    "        audioCutoffOptimizer.step()\n",
    "        embCutoffOptimizer.step()\n",
    "        \n",
    "\n",
    "    # Print the accuracy of the embedding model\n",
    "    splitValAcc = checkAccuracy(loader_val, splitModel, \"split\")\n",
    "    splitTrainAcc = checkAccuracy(loader_train, splitModel, \"split\")\n",
    "    print(\"\\nSPLIT MODEL:\")\n",
    "    print(\"Epoch %d: %.4f val accuracy\" % (e, splitValAcc))\n",
    "    print(\"Epoch %d: %.4f train accuracy\\n\" % (e, splitTrainAcc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
